[
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writings",
    "section": "",
    "text": "My Perspectives on UX Research\n\n\n\nAs a PhD candidate specializing in UX research and human-computer interaction, I often find myself reflecting on the cognitive, emotional, and social dynamics that shape how people interact with systems. These articles are not formal papers or academic publications—they are personal reflections, extensions of my reading, experimentation, and continuous inquiry.\n\n\n\nHow do we choose methods that serve people and not just metrics?\n\n\nHow does cognitive psychology inform usable, inclusive design?\n\n\nHow can we ensure our work remains critical, ethical, and human?\n\n\n\nThese pieces are rooted in my evolving practice as a researcher. I write not as an authority, but as a fellow inquirer—thinking aloud, and sometimes in public.\n\n\n\n\n\n\nHow to Select the Right Research Method for Your UX Project\n\n\nResearch Ethics\n\n\nEvery UX research project begins with a question. And behind that question lies another, often more difficult one — how should I go about answering it?\n\n\nSelecting the right research method isn’t just a matter of logistics—it’s a matter of epistemology. It reflects what we believe is knowable, how we define validity, and what kinds of insights we value.\n\n\nRead more \n\n\nIn applied UX research, these decisions are often shaped by constraints—time, stakeholder expectations, available users. But that doesn’t mean we abandon rigor. Instead, we adapt with intentionality.\n\nMethods Reflect the Kind of Answers You Want\nLet’s start with the basics. If your question is exploratory—“What are users trying to do here?” or “Why do they drop off at this step?”—you likely need qualitative methods: user interviews, contextual inquiry, ethnographic field notes, think-aloud protocols. These methods give you rich narratives and uncover what metrics alone won’t show.\nOn the other hand, if your question is evaluative—“Did this redesign reduce task time?” or “How satisfied are users post-launch?”—you might need quantitative methods: surveys, A/B testing, heatmaps, clickstream analysis. These give you scale, structure, and statistical confidence.\nMost projects benefit from a bit of both. But the danger lies in defaulting to one because it’s convenient. Just because you can run a survey doesn’t mean you should. The method must match the question, not just the budget.\n\n\nFrom Hypothesis to Approach\nOne of the most helpful exercises I’ve developed for myself is to write the research question in plain language, followed by a hypothesis (if applicable), then ask: what kind of evidence would challenge or support this?\nFor example:\n\nQuestion: Why are users abandoning the profile setup page?\nHypothesis: The form is too long and confusing.\nNeeded evidence: Observational data, user narratives, possibly usability test metrics.\n\nThis framing prevents method drift. It reminds me that the purpose isn’t to run a test—it’s to learn something real about users.\n\n\nConstraints Are Not Excuses\nIt’s easy to say “we only had a week” or “we only had five users” as if these limitations invalidate the work. They don’t. But they do shape what claims we can responsibly make.\nWhen I worked on a constrained mobile testing project, we had just three days and no lab access. We set up remote task tests and followed them with brief interviews. The findings were narrow—but actionable. I made sure to document the limitations and frame the results as directional, not definitive.\nThis is key: method selection doesn’t just involve choosing what to do. It involves being transparent about what that choice allows you to claim—and what it doesn’t.\n\n\nDon’t Confuse Tools with Methods\nI’ve often seen teams confuse usability testing software or survey platforms with the methods themselves. Tools are enablers, not frameworks. Knowing how to use Optimal Workshop doesn’t mean you’ve chosen the right kind of card sort. Running a test in Maze doesn’t mean you’ve conducted a rigorous usability evaluation.\nChoosing a method means thinking about data types, user context, ethical implications, and analytical paths. It’s a design decision, not a plug-and-play action.\n\n\nA Word on Hybrid Designs\nSometimes, the best research design blends methods. I’ve combined intercept surveys with follow-up interviews. I’ve run diary studies that culminated in live usability walkthroughs. These combinations allow me to see behavior from multiple angles: what people say, what they do, and how they interpret it.\nBut hybrid designs must be integrated with care. Mixing methods isn’t about doing more for the sake of volume. It’s about cross-validating insights and giving each method space to breathe.\n\n\nThinking Through Cognitive Load\nSomething I bring from my cognitive psychology background is awareness of participant burden. Every research method imposes a cognitive load. A five-step usability test with a think-aloud protocol can be mentally exhausting. A diary study that requires twice-daily entries must be designed with empathy for attention and recall limitations.\nWhen selecting a method, I ask:\n\nWhat kind of memory will this require?\nIs this a reflective or real-time task?\nWill users be multitasking or focused?\n\nGood method design isn’t just about extracting data—it’s about supporting users as they share it.\n\n\nMethod as Advocacy\nFinally, I see method selection as a subtle form of advocacy. When you choose to include marginalized users, when you design for low-bandwidth access, when you take the extra time to translate a survey into a user’s first language—you’re making a claim about whose experience matters.\nIn this sense, choosing a method is never neutral. It’s always political. And that’s okay—so long as you’re aware of it.\n\n\nIn Closing\nIf there’s one thing I’ve learned, it’s this: methods are not mere tools. They are commitments. They define the shape of our understanding and the quality of our empathy.\nSelect them not just with efficiency, but with care.\n\n\n\n\n\n\n\nThe Psychology Behind Creating a User-Centered Design\n\n\nResearch Ethics\n\n\nUser-centered design is often discussed in terms of empathy, usability, and accessibility. These are all essential, but in practice, many “user-centered” experiences still fall short—not because they lack intention, but because they ignore something more fundamental — how the mind actually works.\n\n\nCognitive psychology offers us not just a language for talking about users, but a framework for understanding their limitations, capabilities, and mental patterns.\n\n\nRead more \n\n\nIf design is the act of shaping interactions, then psychology is the raw material we’re shaping around.\nAs a researcher working at the intersection of HCI and cognition, I’ve come to see user-centered design not as a vague value system, but as a psychologically grounded commitment: designing systems that align with how people perceive, decide, remember, and feel.\n\nWorking Memory and the Myth of the “Rational User”\nThe human brain is extraordinarily powerful—but it’s not built for rational, consistent calculation. It’s built for speed, survival, and social interaction. This is why working memory, the mental scratchpad we use for holding temporary information, is incredibly limited—most people can juggle about 4–7 items at once before performance drops dramatically.\nWhen a user interface requires someone to remember a sequence of steps, track multiple conditions, or compare unfamiliar options side by side, it’s effectively demanding more than our minds are built to handle. This is often when frustration sets in.\nGood design works around this. It externalizes memory. It shows you where you are in a process. It provides clear visual hierarchies and groupings. It uses progressive disclosure instead of overwhelming you with options. In short, it offloads mental effort.\n\n\nRecognition Over Recall\nIn cognitive science, there’s a principle that recognition is easier than recall. That’s why multiple-choice questions are generally easier than open-ended ones. We’re much better at recognizing a familiar item than pulling it from memory unassisted.\nThis principle has direct implications for interface design. Systems should present options rather than require users to remember commands. Menus, autocomplete fields, visual cues—all of these support recognition. Compare a command-line interface with a graphical one. The CLI might be more powerful, but the GUI lowers the memory barrier.\nDesigners sometimes underestimate the power of a good label or icon. But when chosen well, these cues anchor cognition. They act as scaffolds for user decision-making.\n\n\nThe Principle of Least Effort\nHerbert Simon, in discussing human decision-making, wrote that people are “satisficers”, not optimizers. We don’t search exhaustively for the best possible option—we go with what seems good enough. This is partly because of attention limits, but also because of cognitive economy: we conserve effort whenever possible.\nThis shows up in everything from form design to information architecture. Users will often pick the most visible or easiest-to-understand option, even if it’s suboptimal. If a signup button is buried beneath five paragraphs of legalese, most people won’t read—they’ll either click blindly or leave.\nUser-centered design means aligning your system’s structure with how people actually navigate options—not how you wish they would.\n\n\nMental Models: The Hidden Maps Users Carry\nA mental model is a user’s internal understanding of how a system works. These models are built from prior experiences, social learning, and interface cues. When a mental model matches the actual system behavior, things feel intuitive. When they don’t, users make errors—or worse, they blame themselves.\nOne example I often use is the “Save” function in desktop applications. For years, this icon was represented by a floppy disk—an obsolete technology that few younger users have actually used. Yet it remained effective because it became a conventional symbol. Mental models aren’t always literal; they’re learned associations.\nThis is why interface consistency matters. If the “back” button sometimes navigates and sometimes closes the app, users will form a confused or fragmented model. That cognitive dissonance creates friction.\nIn research, I often ask users to describe what they think is happening behind the scenes when they take an action. Their answers aren’t always correct, but they reveal how they interpret system logic—and that’s what design has to respond to.\n\n\nEmotion, Confidence, and the Experience of Use\nCognition isn’t cold calculation. It’s deeply tied to emotion. A user who feels confused or overwhelmed may become risk-averse or abandon a task. A user who feels confident is more likely to explore.\nDesigners talk a lot about delight, but from a psychological standpoint, confidence is often more critical. Do users feel in control? Do they feel competent? Is the interface reinforcing their sense of progress?\nProgress indicators, undo functionality, feedback animations—all of these contribute to emotional regulation during interaction. Even something as simple as a “Success” message can ease cognitive dissonance.\nOne of my favorite findings from affective computing is that users who feel slightly uncertain are actually more attentive. That uncertainty is a productive zone—so long as the system guides them through it. Too much friction, and they shut down. But the right level of challenge engages both mind and emotion.\n\n\nDesign as Cognitive Collaboration\nUltimately, I think of user-centered design as a kind of cognitive collaboration. We’re not designing for users so much as designing with their minds in mind. That means understanding not only what they need, but how they think.\nIt also means we must stop blaming users for “not reading”, “clicking the wrong thing”, or “missing the obvious”. If that happens consistently, the problem isn’t their cognition—it’s our design.\n\n\nFinal Thoughts\nCognitive psychology doesn’t replace empathy—it deepens it. It gives us the language and models to anticipate user struggle before it becomes failure. It helps us design systems that feel intuitive not because they’re minimal, but because they match how people actually process the world.\nIf we want to call ourselves user-centered, we must first be mind-aware. Because behind every click, scroll, or tap is a brain—doing its best.\n\n\n\n\n\n\n\nMixed Methods in UX Research: Combining Qualitative and Quantitative Approaches\n\n\nResearch Ethics\n\n\nIn UX research, there’s a persistent temptation to ask: “Which method is better—qualitative or quantitative?” But I’ve come to believe this is the wrong question. Better, perhaps, is to ask: “What dimensions of experience are we missing when we rely on only one?”\n\n\nAs a PhD candidate conducting fieldwork and system evaluation, I frequently move between methods. In practice, I’ve found that real understanding rarely emerges from one type of data alone.\n\n\nRead more \n\n\nQuantitative data can tell you that something is happening; qualitative data often tells you why. Used together, they form a picture that’s not only accurate—but human.\nThis is the promise of mixed methods: not methodological compromise, but epistemological strength.\n\nWhy Mixed Methods?\nMixed methods research is not just about being thorough. It’s about responding to the layered nature of user experience. People act, feel, reflect, and adapt—and those processes unfold across different kinds of evidence.\nSay you’re redesigning a search feature. A usability test might show a 60% task success rate. That’s helpful. But pairing that with interview data might reveal that users don’t understand the terminology used in filters—or that they’re unsure what kind of results they should expect.\nThe quantitative result gives you a signal. The qualitative result gives you a story. When both align, you have validation. When they diverge, you have an opportunity for deeper insight.\n\n\nDesigning a Mixed Methods Study\nGood mixed methods research doesn’t just mash together tools. It requires careful sequencing, sampling, and synthesis.\nThere are several basic models for structuring mixed methods:\n\nSequential Exploratory: Start with qualitative research (e.g., interviews), then use those findings to design a quantitative instrument (e.g., survey).\nSequential Explanatory: Begin with quantitative data to identify trends, then follow up with qualitative research to explain anomalies or user reasoning.\nConcurrent Triangulation: Conduct both types simultaneously and compare findings for convergence or contradiction.\n\nEach approach has tradeoffs. Exploratory studies are rich but time-intensive. Triangulation can highlight inconsistencies but requires careful analytical integration. What matters is not the format—it’s the clarity of purpose. Why are you combining methods? What will each add?\n\n\nIntegrating Findings: More Than Just Juxtaposition\nOne of the most common pitfalls I see—especially in industry—is using mixed methods in parallel without actually integrating the findings. A team runs a usability test, sends out a survey, and presents both results in separate decks. That’s not synthesis—it’s colocation.\nTo truly integrate findings, we need to ask:\n\nWhere do the two data types reinforce each other?\nWhere do they contradict, and why?\nHow does each data type shift our interpretation of the other?\n\nFor instance, imagine a pattern where users complete a task successfully (as measured by logs) but rate the experience as frustrating. Without qualitative input, this dissonance might be dismissed. But user narratives might reveal that although the interface “worked,” it felt unintuitive or required workaround strategies.\nThis kind of layered analysis helps product teams avoid shallow conclusions and surface root causes—not just symptoms.\n\n\nChoosing What to Measure and Why\nThe strength of mixed methods depends heavily on what you choose to measure—and how well those metrics align with user behaviors and mental models.\nA key insight from cognitive research is that people don’t always report their own experiences accurately. We rationalize, forget, or misunderstand our actions. That’s why pairing observed behavior with self-report is so powerful. You’re comparing what users say they do with what they actually do.\nIn one project, I noticed a significant drop-off at a critical conversion step. Behavioral data suggested confusion, but interviews revealed something more emotional: users didn’t trust the company enough to commit. The issue wasn’t UI design—it was perceived credibility.\nHad I relied on numbers alone, I would’ve redesigned the interface. With mixed methods, we improved the brand voice and user onboarding flow instead—and retention increased.\n\n\nMixed Methods and Stakeholder Communication\nIn product teams, mixed methods research is also an asset for communication. Different stakeholders respond to different forms of evidence. A CFO might value statistical trends; a product designer might need a user quote to spark empathy.\nWhen I present mixed methods findings, I structure them in layers:\n\nTop-line takeaways (what we learned)\nSupporting evidence (a metric and a story)\nImplications (what we do next)\n\nThis not only strengthens trust in the research but models rigorous, human-centered thinking. It demonstrates that insight isn’t just about data—it’s about meaning.\n\n\nNavigating Tensions Between Methods\nOf course, combining methods isn’t always smooth. Quantitative tools often require scale; qualitative ones require depth. They run on different timelines and often pull in opposite directions: one toward generalization, the other toward particularity.\nThis tension is productive—but only if acknowledged. It forces you to confront the complexity of human behavior. It invites humility: your job isn’t to simplify—it’s to understand.\nI’ve found that the best way to manage this tension is through alignment. Ensure that your methods are working on the same problem, with the same user group, during the same timeframe. This coherence makes integration possible—and valuable.\n\n\nFinal Thoughts\nMixed methods UX research is not just about using multiple tools. It’s about cultivating a mindset—one that values pluralism, context, and the layered nature of experience.\nIt reminds us that users are not data points, nor anecdotes. They are people—multifaceted, inconsistent, adaptive—and understanding them requires more than one lens.\nIn a world of increasing complexity, mixed methods research offers a way to see more clearly—not just what’s happening, but what matters.\n\n\n\n\n\n\n\nThe Role of Data Analysis in Modern UX Research\n\n\nResearch Ethics\n\n\nUX research is changing. As digital products generate more real-time behavioral data, researchers are increasingly expected to analyze, interpret, and communicate complex datasets. Metrics are no longer the exclusive domain of data scientists—they are central to the daily practice of UX.\n\n\nData analysis can elevate UX research. It can reveal hidden patterns, validate design decisions, and highlight pain points invisible to qualitative methods alone.\n\n\nRead more \n\n\nBut without care, data can also be misleading. It can flatten the user’s experience into numbers and sever the human from the behavior.\nIn this article, I want to reflect on how I’ve learned to navigate data analysis as part of my UX research practice—not just technically, but philosophically.\n\nWhy Data Analysis Matters in UX\nThe interface between humans and technology generates an enormous volume of traceable behavior: clicks, taps, scrolls, exits, searches, time on task, paths abandoned, and features ignored. Within this noise is a signal—sometimes several.\nData analysis helps us answer core UX questions:\n\nWhat behaviors are users performing most often?\nWhere are users hesitating or dropping off?\nWhat actions correlate with successful task completion?\nHow does behavior differ across devices, segments, or time?\n\nThese are powerful questions, and they often lead to decisions that affect thousands or millions of users. That’s why it’s critical that UX researchers engage not just with the output of analytics—but with its assumptions.\n\n\nWhat Counts as “Data”?\nWhen people say “data” in tech environments, they often mean quantitative metrics—numbers collected from logs or events. But in UX, data is broader.\nA researcher’s field notes are data. So are interview transcripts, open-ended survey responses, screenshots of user flows, emotional expressions captured during usability tests. These are data points too—rich, narrative, contextual.\nWhat analysis allows us to do is move across abstraction levels. We might go from a single user quote to a pattern in 100 responses. From an observed hesitation on a checkout screen to a 32% abandonment rate in funnel analytics. The analyst’s job is to connect these layers responsibly.\n\n\nCommon Analytical Approaches in UX\nIn my own work, I use a range of data analysis methods depending on the question, the scope, and the data type.\nFor behavioral and usage data, I often rely on:\n\nDescriptive statistics: means, medians, standard deviations. These provide a sense of spread and central tendency.\nFunnels and pathing analysis: where users start, where they drop, and what diverging flows emerge.\nSegmentation: comparing behavior across different user types, devices, or time periods.\nRetention curves and cohort analysis: useful for understanding long-term engagement.\n\nFor qualitative data, my analysis involves:\n\nThematic coding: identifying recurring ideas, emotions, or barriers.\nAffinity mapping: grouping similar responses to uncover structures.\nSentiment analysis: for open-ended survey fields or transcripts.\n\nIncreasingly, I find myself working at the boundary—where numbers and narratives intersect. A usage pattern becomes meaningful when paired with a quote. A high Net Promoter Score (NPS) becomes more useful when we understand why users scored it that way.\n\n\nData Literacy Is a UX Skill\nBeing data-literate doesn’t mean you need to run regressions or build dashboards from scratch. But it does mean you can:\n\nAsk the right questions of your data\nUnderstand common pitfalls (like survivorship bias or Simpson’s paradox)\nTranslate results into design-relevant insights\nChallenge misleading interpretations\n\nFor example, I once worked with a product team excited about an increase in feature engagement. The data showed more users clicking into a newly released tool. But after examining time-on-task and follow-up actions, I noticed that most users exited within five seconds and never returned. The spike was driven by curiosity—not utility.\nHad we stopped at surface-level analysis, we would have misread failure as success.\n\n\nData Without Context Is Dangerous\nOne of the lessons I’ve had to learn repeatedly is this: behavioral data tells you what, not why. Without qualitative context, it’s easy to make assumptions that sound plausible but lack grounding.\nIf users aren’t finishing a form, it could be due to confusion. Or mistrust. Or a technical bug. Or a misplaced input field.\nThis is why I rarely treat data analysis as standalone. I always try to pair it with user narratives, even if only a small sample. A single quote, paired with a trend line, often carries more persuasive power than either alone.\n\n\nEthical Considerations in UX Data\nAs UX researchers, we are stewards of user information. That means we must think carefully about how we analyze data—and what we choose to measure.\nTracking user behavior without consent is unethical. Over-collecting personal data—even if it’s “anonymized”—raises questions of privacy and agency. Designing metrics that serve company OKRs but not user needs distorts both research and ethics.\nI believe good UX research asks: are we using this data to serve users—or just to manage them?\n\n\nData Analysis as Interpretation, Not Just Computation\nAt its core, data analysis in UX is not about math. It’s about meaning.\nWhen I look at a spike in helpdesk tickets, or a drop in activation rate, I’m not just crunching numbers. I’m telling a story about frustration, confusion, hesitation. These numbers are fingerprints of human experience.\nThe best UX researchers don’t treat analysis as a separate phase. They integrate it into every step: designing studies that yield interpretable results, asking questions shaped by context, and presenting data in a way that’s both rigorous and empathetic.\n\n\nFinal Thoughts\nModern UX research is inseparable from data. But it’s up to us to ensure that data doesn’t eclipse users themselves.\nWe must treat analysis not as a path to certainty, but as a way to listen at scale—to hear, in aggregate, the murmur of human experience as it moves through our designs.\nData is never just a dashboard. It’s a mirror. And how we read it determines what kind of designers—and researchers—we become.\n\n\n\n\n\n\n\nEthical Considerations in UX Research\n\n\nResearch Ethics\n\n\nEthics in UX research is not just about institutional review boards or legal compliance. It’s about the impact of our work—how we gather information, who we include or exclude, and how our findings affect real people.\n\n\nAs a researcher, I’ve come to see ethics not as a static code but as a set of tensions to be navigated—between learning and respecting, between curiosity and care, between insight and harm.\n\n\nRead more \n\n\nMany of the most consequential ethical decisions don’t happen in formal reviews. They happen in planning meetings, recruitment emails, usability sessions, data exports, and stakeholder debriefs.\nThis article explores the ethical questions I return to most in my work—not just to check boxes, but to guide the kind of researcher I want to be.\n\nInformed Consent Is a Process, Not a Checkbox\nThe first principle we’re taught is informed consent. But in practice, consent often becomes procedural—a form to sign or a pop-up to click. Real consent, however, is relational. It’s about ensuring that participants understand what they’re sharing, what it will be used for, and how it may affect them.\nIn corporate settings, I’ve often had to explain to stakeholders why “implicit consent” from product use isn’t enough. Just because a user clicked “I agree” doesn’t mean they knew their behavior would be analyzed in a research report.\nIn my work, I try to slow this moment down. I offer plain-language explanations. I let participants ask questions. I clarify that they can skip questions or withdraw entirely. And I remind teams that ethical research isn’t just about access—it’s about trust.\n\n\nWhose Voices Are We Listening To?\nRepresentation is another ethical concern that goes beyond recruitment quotas. It’s about who gets to shape the understanding of a problem.\nUX research often centers what’s easy to study: high-frequency users, English speakers, urban populations with reliable Wi-Fi. But that leaves out people at the edges—those with disabilities, those using assistive technologies, those with limited digital literacy, or those who’ve been excluded from “default” user profiles.\nEvery time we exclude a group for convenience, we re-inscribe a digital status quo that privileges some and marginalizes others.\nEthical research means going the extra mile to include harder-to-reach users—not just for equity, but for better design. Often, the edge cases show us what’s broken or fragile in the system for everyone else.\n\n\nThe Power Dynamics in Asking Questions\nThere’s an inherent power dynamic in research: we ask, they answer. Even in user-friendly interviews, participants often want to please. They perform usefulness. They mirror back what they think we want to hear.\nThat’s why I try to remain aware of how I frame questions, how I respond to uncertainty, and how much I talk. Silence can be uncomfortable—but it often allows the participant to shape the narrative.\nI’ve also learned to pay attention to how I introduce myself. Am I emphasizing expertise in a way that makes the participant feel smaller? Or am I inviting them to co-create meaning?\nEthical research is dialogic. It’s about listening as an act of respect.\n\n\nData Stewardship: Beyond Deletion Policies\nWe often talk about ethical data use in terms of deletion, storage, and encryption. These are essential. But equally important is the question of interpretation.\n\nAre we interpreting this data in a way that reflects users’ intent?\nAre we using it to improve their experience, or merely to optimize engagement?\nAre we anonymizing, but still extracting in ways that feel exploitative?\n\nIn one project, I noticed that repeated user session recordings were being reviewed without context—just for “general inspiration.” It made me uncomfortable. The users hadn’t consented to that level of scrutiny. So I advocated for limiting playback to sessions tied to active usability questions, and for deleting recordings within a set window.\nWe can’t just protect data from breaches—we have to protect it from misuse, even internally.\n\n\nEthical Stakeholder Management\nUX researchers often serve as intermediaries between users and powerful institutions. This can put us in difficult positions.\nWe may find ourselves asked to design studies that feel extractive, to downplay uncomfortable findings, or to “massage” quotes to support a feature roadmap. Sometimes we’re asked to simplify nuance in the name of clarity.\nI believe ethical practice requires some degree of moral courage. It means pushing back—gently but firmly—when research is used to justify decisions that may harm or mislead. It means refusing to reduce users to KPIs when the real issue is their wellbeing.\nSometimes, the most ethical thing a researcher can do is to say: “We need to pause and reframe the question.”\n\n\nEthics in Method, Not Just Topic\nA common misconception is that ethical risk only arises in sensitive topics—mental health, financial vulnerability, trauma. But even “neutral” topics can carry ethical implications.\nFor example, in a study about navigation behavior, I noticed a participant becoming visibly anxious. She didn’t want to seem “bad at tech.” She was performing competence. That moment reminded me that being observed is itself a stressor. It affects people emotionally—even in mundane contexts.\nEthical research design means thinking about:\n\nThe time demands placed on participants\nThe emotional toll of certain tasks\nThe tone of feedback mechanisms\n\nWe don’t just study people—we affect them. That’s a responsibility we should never take lightly.\n\n\nFinal Reflections\nEthics in UX research isn’t just about doing no harm. It’s about practicing care—toward participants, their data, and the truths they entrust us with.\nI don’t always get it right. None of us do. But I try to stay in conversation—with myself, with colleagues, and with the people I study. Ethics, after all, is not a fixed destination. It’s a practice of ongoing attention.\nAnd if we truly believe in user-centered design, then ethics isn’t separate from research—it is the research.\n\n\n\n\n\n\n\nFrom Research to Action: Translating UX Insights into Design Decisions\n\n\nResearch Ethics\n\n\nEvery UX researcher knows the feeling: you’ve just wrapped a well-run study. The findings are thoughtful, the insights rich, the implications clear. But then—nothing. No changes are made. The design direction stays the same. The report is skimmed, perhaps nodded at, then filed away.\n\n\nThe truth is, conducting great research is only half the job. The other half—and arguably the harder part—is making that research usable: understood by stakeholders, embraced by designers, acted on by teams.\n\n\nRead more \n\n\nTranslating research into design decisions is not an output problem; it’s a communication, trust, and timing problem.\nThis article explores how I’ve come to approach this translation work: not as an afterthought, but as a core part of my research practice.\n\nInsights Are Only Useful if They’re Heard\nAn insight is a bridge—it connects what users experience to what teams can act on. But too often, insights are locked in long documents, written in language that’s either too academic or too vague. Teams don’t ignore research because they don’t care. They ignore it because it doesn’t speak their language or solve their problem now.\nEarly in my doctoral research, I was guilty of over-documentation. I would spend days coding transcripts, refining themes, writing elegant findings sections. Then I’d present them to a product team already knee-deep in sprints. I learned, quickly, that relevance beats thoroughness. Timeliness beats polish.\nNow, I build communication into the research process from the start. I ask: Who needs to hear this? When will they be most receptive? What format will move them?\n\n\nDesign Doesn’t Speak Research. Translate Accordingly.\nUX researchers often speak in frameworks: mental models, task flows, journey stages, friction points. Designers think in screens, states, and interactions. Engineers think in logic and structure. Product managers think in metrics and OKRs.\nThis isn’t a hierarchy—it’s a translation challenge.\nI’ve found that pairing research with design artifacts accelerates adoption. For instance:\n\nInstead of just reporting a pain point, I’ll mock up how it could look in the UI.\nInstead of listing findings, I’ll link them directly to screens in the design file.\nInstead of creating standalone personas, I’ll work with the designer to embed behavior insights directly into component documentation.\n\nThe goal is not to “dumb down” the research. It’s to shape it so it enters the workflow without friction.\n\n\nTiming Matters as Much as Insight\nEven the best research can be rendered moot by bad timing. If findings arrive after design decisions are made—or too early to feel relevant—they risk irrelevance.\nThat’s why I aim to insert research into the rhythm of design, not on the edges. I join planning meetings, offer just-in-time insights, and shape research plans around sprint cycles. I let go of perfection in favor of presence.\nSometimes, I’ll run small, scrappy tests just to get directional input before a big design kickoff. Other times, I’ll revisit older research and reframe it in terms that match current priorities.\nUX research is not a static archive—it’s a living dialogue with the present moment.\n\n\nFraming: From Findings to Meaning\nThere’s a subtle but powerful difference between a “finding” and an “insight.”\n\nA finding is: Users had trouble locating the filter menu.\nAn insight is: Users expect filters to appear only after they begin searching. The current placement breaks that mental model.\n\nFindings describe what happened. Insights explain why it matters. This difference is critical when trying to influence design decisions.\nI use a simple structure to move from raw observation to design implication:\n\nWhat did we observe? (e.g., 5 of 6 users skipped over a key CTA)\nWhat does this suggest? (e.g., the CTA’s placement contradicts scanning patterns)\nWhat should we consider? (e.g., relocating the CTA to the primary visual flow)\n\nThis structure helps stakeholders not just know what we found—but care.\n\n\nThe Researcher as Facilitator, Not Just Analyst\nOver time, I’ve come to see the UX researcher not just as a knowledge generator, but as a facilitator of shared understanding.\nThis might mean:\n\nRunning workshops where stakeholders interact directly with raw data\nCreating “research walls” or digital spaces where team members can see quotes, screenshots, and journey maps evolve\nFacilitating sketch sessions informed by real user stories\n\nThese practices don’t just transfer insights—they build empathy. They collapse the gap between “users” and “us”.\nAnd they signal that research isn’t a report—it’s a practice of collective meaning-making.\n\n\nDealing with Resistance\nSometimes, even clear, well-timed, actionable insights are dismissed. Teams may already be overcommitted. Stakeholders may be defensive. The findings may contradict assumptions that are politically risky to challenge.\nIn these moments, I remind myself: research doesn’t need to win the argument—it needs to keep the door open.\nI document findings clearly, archive quotes and recordings, and make it easy to revisit the data later. Often, the same stakeholder who rejected the insight will return weeks later when the problem persists.\nTruth has a way of resurfacing—especially when it’s grounded in real user experience.\n\n\nFinal Thoughts\nIf UX research is about understanding, then research translation is about making that understanding actionable. It’s about advocacy, timing, and thoughtful storytelling. It’s where research earns its relevance.\nThe most powerful research insights don’t just describe users—they move teams. They change roadmaps. They alter flows. They bring a bit more clarity, humanity, and nuance into how we design the systems people live with every day.\nAs a researcher, I’ve come to believe that insight alone is not enough. Insight must travel—and our job is to build the path."
  },
  {
    "objectID": "casestudies.html",
    "href": "casestudies.html",
    "title": "Case Studies",
    "section": "",
    "text": "Below are detailed case studies that demonstrate my research approach, methodologies, and impact. Each case study highlights different aspects of my UX research expertise across various domains."
  },
  {
    "objectID": "casestudies.html#enhancing-data-accessibility-through-multimodal-ux-for-blv-users-maidr-project",
    "href": "casestudies.html#enhancing-data-accessibility-through-multimodal-ux-for-blv-users-maidr-project",
    "title": "Case Studies",
    "section": "Enhancing Data Accessibility through Multimodal UX for BLV Users (MAIDR Project)",
    "text": "Enhancing Data Accessibility through Multimodal UX for BLV Users (MAIDR Project)\n\nResearch Ethics Conversational AI Accessibility Mixed Methods Research Data Visualization\nJanuary 2024 - Present\n\nProject Context\nWhen I joined the MAIDR team, a pressing research gap was immediately visible:\nWhile large language models (LLMs) could generate text descriptions of complex data visualizations, there was no structured understanding of how Blind and Low Vision (BLV) users would interact with, trust, customize, or verify these outputs.\nEqually important, there was minimal insight into how tactile, auditory, and braille modalities independently supported users’ cognitive and emotional engagement with data.\nAs a mixed-methods UX researcher, I positioned my work around filling these gaps systematically through multi-phase, user-centered investigations.\n\n\nResearch Objective\n\nHow can multimodal and AI-driven systems enable independent, trustworthy, and cognitively accessible data interpretation for BLV users?\n\n\n\nResearch Strategy Overview\nGiven the complexity, I structured the work into two sequential studies that informed each other:\n\n\n\n\n\n\n\n\nStudy\nKey Focus\nMethods Employed\n\n\n\n\nStudy 1\nUser interaction with AI-generated descriptions (maidrAI)\nSemi-Structured Interviews, Thematic Analysis, AUS Survey\n\n\nStudy 2\nUser performance across tactile, sonified, and braille data representations\nGesture Analysis, Demographic Survey, Statistical Analysis (Friedman Test)\n\n\n\n\n\n\nStudy 1: Evaluating AI-Generated Multimodal Descriptions (maidrAI)\n\nProblem Framing\nDespite LLMs’ potential to describe data, trust and personalization remained critical unknowns:\n\nWould users accept the AI’s interpretation at face value?\n\nHow might users want to prompt, edit, or verify descriptions?\nCould verbose AI outputs increase cognitive fatigue?\n\nI hypothesized that customization and validation mechanisms would be key to successful adoption.\n\n\nMethodological Approach\nMixed Methods Design:\nI blended qualitative exploration with quantitative usability metrics to triangulate user experience:\n\n\n\n\n\n\n\n\nData Type\nMethod\nPurpose\n\n\n\n\nQualitative\nSemi-Structured Interviews\nCapture personalization needs, verification strategies\n\n\nQualitative\nThematic Analysis\nDerive design principles from user narratives\n\n\nQuantitative\nAccessible Usability Scale (AUS)\nQuantify satisfaction, usability, cognitive load\n\n\n\n\n\nSemi-Structured Interviews\nAfter participants explored the maidrAI system, I conducted semi-structured interviews focusing on:\n\nHow they adapted or prompted the AI\nStrategies they used to verify output trustworthiness\nPreferences around description styles and length\n\nEmergent Themes:\n\nLayered Summaries: Users preferred quick summaries with the option to drill down into detail.\nVerification Rituals: Users routinely “double-checked” AI-generated information.\nCognitive Load: Verbosity increased mental effort, especially when navigating complex datasets.\n\n\n\n\nAccessible Usability Scale (AUS) Survey\nQuantitative triangulation confirmed qualitative insights:\n\nCognitive Overload: Participants rated maidrAI lower on cognitive simplicity.\nTask Completion: Higher scores for independence but lower scores for efficiency.\n\nThis validated the urgent need for adaptive AI responses based on user cognitive bandwidth.\n\n\nKey Insights from Study 1\n✅ AI outputs must be modular and customizable for BLV users to maintain autonomy.\n✅ Trust-building in AI requires transparency and control, not just accuracy.\n✅ Cognitive load is a UX barrier, and reducing it should be a primary design goal.\n\n\n\nStudy 2: Recognizing Statistical Properties Through Multimodal Interaction\n\nProblem Framing\nBeyond textual descriptions, how well could BLV users interpret statistical properties (like skewness and modality) through tactile, sonified, or braille data representations?\nI reasoned that performance differences across modalities could inform adaptive, user-selectable multimodal systems.\n\n\nMethodological Approach\nBehavioural + Performance Data\nI expanded from self-reported data to observable behaviours and performance outcomes.\n\n\n\n\n\n\n\n\nData Type\nMethod\nPurpose\n\n\n\n\nBehavioural\nGesture Analysis\nMap tactile and braille exploration strategies\n\n\nBehavioural\nPlayback Tracking\nAnalyze auditory navigation behaviour\n\n\nPerformance\nAccuracy, Confidence, Response Time\nEvaluate task success across modalities\n\n\nStatistical\nFriedman Test\nAssess modality differences significance\n\n\n\n\n\nGesture Analysis and Playback Tracking\n\nDesigned 9 histograms representing different statistical shapes.\nCreated tactile and braille diagrams (swell-form paper) and sonified versions.\nRecorded:\n\nHand movement patterns (start points, looping, speed changes)\nSonification playback repetitions and edits\n\n\nBehavioural Findings:\n\nTactile diagrams promoted methodical, slow scanning patterns.\nSonification prompted rapid comparisons but sometimes missed fine details.\nBraille readers relied heavily on numeric precision but struggled with “big-picture” trends.\n\n\n\n\nQuantitative Performance Analysis\nUsing the Friedman Test, I compared user performance:\n\n\n\nModality\nAccuracy\nResponse Time\nConfidence\n\n\n\n\nTactile\nModerate\nSlow\nHigh\n\n\nSonification\nModerate\nFast\nModerate\n\n\nBraille\nHigh\nSlow\nModerate\n\n\n\nInterpretation:\n\nNo statistically significant difference in raw accuracy.\nSignificant modality-driven differences in confidence and speed.\nTactile representations improved comprehension but demanded higher cognitive effort.\n\n\n\n\nIntegrating Findings into Design\nGiven user diversity, I concluded that a static one-size-fits-all modality would fail.\nInstead, I designed a dynamic dashboard prototype that lets users:\n\nUpload datasets\nChoose between tactile, sonified, or text outputs\nCustomize verbosity, feedback, and navigation style\n\n🔗 Interactive Dashboard Prototype\n\n\n\nReflections on Research Process\n\nWhat Worked\n\nSequential mixed methods let qualitative insights directly inform quantitative design.\nBehavioural observation + performance metrics revealed subtleties not captured in self-report.\nCollaborative design with BLV researchers and users grounded every decision in lived experience.\n\n\n\nChallenges\n\nManaging cognitive load across different modalities required delicate balancing.\nInterpreting non-verbal behavioural data (gestures, replays) demanded careful cross-validation.\n\n\n\nPersonal Learning\nThis project solidified my belief that trust, control, and personalization must drive accessible AI design — especially when navigating cognitive and emotional complexity in assistive tech.\n\n\n\nFinal Outcome\n✅ Generated empirical insights on modality-specific strengths and weaknesses.\n✅ Designed a user-driven dashboard enabling customizable multimodal data access.\n✅ Contributed to the broader mission of equitable, ethical, and autonomous AI accessibility solutions.\nBack to Projects"
  },
  {
    "objectID": "casestudies.html#designing-accessible-vr-exergames-for-blind-and-low-vision-blv-users",
    "href": "casestudies.html#designing-accessible-vr-exergames-for-blind-and-low-vision-blv-users",
    "title": "Case Studies",
    "section": "Designing Accessible VR Exergames for Blind and Low Vision (BLV) Users",
    "text": "Designing Accessible VR Exergames for Blind and Low Vision (BLV) Users\n\n\nResearch Ethics Virtual Reality Haptic Design Spatial Audio Physical Engagement\nMarch 2023 - January 2024\n\nProject Context\nVirtual Reality (VR) offers incredible potential for immersive physical engagement.\nYet, for Blind and Low Vision (BLV) users, VR often remains an exclusionary medium, heavily reliant on visual cues.\nWhen I joined this project, I recognized a powerful opportunity:\nCould we create skill-based, independently playable VR exergames for BLV users, using multimodal (audio + haptic) interaction?\n\n\nResearch Objective\n\nHow can multimodal sensory feedback enable accessible, skill-driven VR sports gameplay for BLV users, without compromising realism or agency?\n\n\n\nResearch Strategy Overview\nGiven the complexity of real-time movement, feedback, and immersion in VR, I structured research into two major phases:\n\n\n\n\n\n\n\n\nStudy\nKey Focus\nMethods\n\n\n\n\nStudy 1\nAccessible VR Table Tennis Simulation\nParticipatory Co-Design, Iterative Usability Testing, Spatial Audio + Haptic Design\n\n\nStudy 2\nAccessible VR Boxing for Physical Engagement\nParticipatory Co-Design, Game Analytics, Physical Activity Measurement, Qualitative Immersion Interviews\n\n\n\n\n\nStudy 1: Accessible VR Table Tennis Game\n\nProblem Framing\nCurrent VR adaptations for accessibility often introduce “patches” after development.\nI hypothesized: True BLV accessibility demands integrated design from the outset — making audio and haptics foundational, not secondary.\n\n\nMethodological Approach\nMixed Methods Participatory Research:\n\n\n\n\n\n\n\n\nData Type\nMethod\nPurpose\n\n\n\n\nQualitative\nCo-Design Sessions with BLV Players\nShape mechanics, feedback loops\n\n\nBehavioural\nUsability Observations\nIdentify interaction barriers\n\n\nSensory\nSensory Feedback Calibration\nOptimize spatial audio + haptic timing\n\n\nIterative\nRapid Prototyping + Testing\nAgile UX improvements\n\n\n\n\n\nParticipatory Co-Design\nI conducted weekly participatory sessions with BLV co-designers over four months:\n\nUsers tested early builds and prototypes.\nFeedback loops focused on sensory clarity, reaction time, and situational awareness.\n\nEmergent Priorities:\n\nSpatial audio must clearly track ball movement — pitch, distance, velocity.\nHaptic feedback must guide paddle contact strength and timing.\nReduce visual noise to minimize cognitive clutter for low-vision players.\n\n\n\nSensory Integration Design\n\n\n1. Spatial Audio (Unity Engine)\n\nBall tracking was tied to real-time 3D audio positioning.\nDoppler effect simulation conveyed ball speed and directional drift.\nPaddle had locational audio buzz to help players orient.\n\n\n\n2. Haptic Feedback (bHaptics Studio)\n\nPaddle-ball contact strength modulated vibration intensity.\nIncoming ball proximity triggered progressive pulses.\nMissed shots activated distinct low-frequency vibration for corrective feedback.\n\n\n\n\nUsability Testing and Iteration\nDuring testing: - Players initially overreacted to Doppler shifts. - Some participants misjudged ball bounce height due to uniform audio cues.\nDesign Refinements: - Added low-pass filters for bounce vs. paddle impact sound. - Introduced boundary haptics to prevent table disorientation.\n\n\nOutcomes from Study 1\n✅ Created a realistic, skill-based VR Table Tennis experience for BLV players without visual reliance.\n✅ Established foundational multimodal sensory design principles for accessible VR sports.\n\n\n\nStudy 2: Accessible VR Boxing Game\n\nProblem Framing\nTable Tennis enabled technical reaction skills — but lacked sustained physical exertion.\nI hypothesized: VR Boxing could offer a more physically engaging experience, if designed with accessible non-visual navigation.\n\n\nMethodological Approach\nFull Mixed Methods Study:\n\n\n\n\n\n\n\n\nData Type\nMethod\nPurpose\n\n\n\n\nBehavioural\nCo-Design Iterations\nOptimize opponent tracking, haptic hit feedback\n\n\nQuantitative\nPhysical Activity (PA) Tracking\nMeasure exertion levels (Heart Rate, Energy Use)\n\n\nQuantitative\nImmersion Assessment (ITC-SOPI Survey)\nEvaluate presence and emotional engagement\n\n\nQualitative\nImmersion Interviews\nSurface emotional responses, strategy variations\n\n\n\n\n\nCo-Design and System Implementation\n\nDesigned a clock-based spatial audio cueing system (e.g., “attack incoming at 3 o’clock”).\nIntegrated adaptive difficulty settings (reaction speeds, cue intensity) based on user skill.\nDeveloped real-time haptic response tied to punch impact and dodges.\n\n\n\n\nEvaluation: Physical Activity and Immersion\n\n1. Physical Effort Metrics\n\nMonitored Heart Rate across sessions.\nCompared physical activity expenditure between two participants (different gameplay strategies).\n\n\n\n\n2. User Immersion (ITC-SOPI Survey)\n\nParticipants scored high on:\n\nSense of physical presence\nPerceived realism\nEngagement and emotional immersion\n\n\n\n\n3. Qualitative Feedback\n\nPlayers felt empowered and independent.\n“The audio made it feel like I was in a real fight,” remarked one participant.\n\n\n\n\nOutcomes from Study 2\n✅ Validated that spatial audio + adaptive haptics can enable physically active, immersive VR fitness for BLV users.\n✅ Demonstrated different play styles (aggressive vs. strategic) across participants, reinforcing the need for flexible difficulty tuning.\n\n\n\nReflections on Research Process\n\nWhat Worked\n\nMultimodal layering created intuitive situational awareness.\nCo-design empowered real user voices throughout mechanical iterations.\n\n\n\nChallenges\n\nClock-cue training curves varied by individual; learning time required.\nManaging physical safety during dynamic movement in a sightless environment demanded extra safeguards.\n\n\n\nPersonal Learning\nTrue VR accessibility is not about replacing vision, but about expanding sensory frameworks intelligently — trusting users’ spatial intelligence beyond sight.\nBack to Projects"
  },
  {
    "objectID": "casestudies.html#clearminds-designing-trustworthy-digital-mental-health-support",
    "href": "casestudies.html#clearminds-designing-trustworthy-digital-mental-health-support",
    "title": "Case Studies",
    "section": "ClearMinds: Designing Trustworthy Digital Mental Health Support",
    "text": "ClearMinds: Designing Trustworthy Digital Mental Health Support\n\nResearch Ethics Mental Health UX Empathic Design Emotional Interfacing  User Research\nMarch 2023 - June 2023\n\nProject Context\nMental health apps often face two core issues:\n- Users struggle to trust generalized, AI-driven recommendations.\n- Tracking emotional progress feels either too mechanical or too overwhelming.\nWhen I conceptualized ClearMinds, I asked:\nCan we create a system that feels structured yet human, scientific yet emotionally validating?\n\n\nResearch Objective\n\nHow can mental wellness platforms balance structure, trust, and emotional nuance to support sustained, meaningful user engagement?\n\n\n\nResearch Strategy Overview\n\n\n\n\n\n\n\n\nPhase\nFocus\nMethods\n\n\n\n\n1\nUnderstand User Pain Points\nPersona Building, Journey Mapping\n\n\n2\nIdeate Structure\nAffinity Diagramming, Storyboarding\n\n\n3\nPrototype and Test\nPaper Wireframing, Low-Fidelity UX Testing\n\n\n4\nRefine and Evaluate\nHigh-Fidelity Prototyping, Accessibility Audits, Usability Testing\n\n\n\n\n\n\nPhase 1: Discovery — Understanding Emotional and Structural Needs\n\nPersona Creation\nThrough early interviews and secondary research, I created core user personas:\n\nTrust-Seeker: Skeptical of digital therapy.\nStructure-Seeker: Wants clear progress pathways.\nFlexibility-Seeker: Needs adaptable emotional tracking tools.\n\n\n\n\nJourney Mapping\nMapped emotional highs and lows across user experiences with existing apps.\nCritical “drop-off” points: - Feeling misunderstood by generic AI feedback. - Overwhelmed by rigid task flows without customization.\n\n\n\nPhase 2: Ideation — Structuring Flexible Support\n\nAffinity Diagramming\nGrouped user needs into thematic clusters:\n\n\n\n\n\n\n\n\nTheme\nImplication\n\n\n\n\n\nTrust & Transparency\nShow users how recommendations are formed\n\n\n\nFlexible Structure\nAllow self-paced, customizable progress tracking\n\n\n\nEmpowerment\nOffer users second-opinion mechanisms and therapist switch options\n\n\n\n\n\n\nStoryboarding\nVisualized use cases where structured guidance adapts dynamically to emotional states.\n\n\n\n\nPhase 3: Prototyping — Testing Core Concepts\n\nPaper Wireframes and Low-Fidelity Prototypes\n\nCreated progress dashboards\nDesigned structured task boards with opt-out flexibility\nDeveloped “therapist transparency” profiles for clearer communication\n\n\n\nUsability Testing\nTested wireframes with real users.\nMajor learnings: - Needed visual metaphors for emotional states (e.g., weather systems). - Mandatory task tracking discouraged engagement; flexibility was crucial.\n\n\n\nPhase 4: Refinement — Building Trust and Accessibility\n\nHigh-Fidelity Prototyping\n\nIntegrated second-opinion features for therapy paths.\nGamified emotional tracking with visual progress markers.\nAdded optionality across every structure (skip, edit, pause).\n\n\n\nAccessibility Audits\n\nEnsured WCAG 2.1 AA compliance.\nDesigned dark mode and dyslexia-friendly font options.\n\n\n\n\n\nReflections on Research Process\n\nWhat Worked\n\nBuilding transparency and control built trust faster than “smarter” AI claims.\nFlexible structure encouraged emotional authenticity and long-term engagement.\n\n\n\nChallenges\n\nBalancing emotional depth with UX simplicity required careful emotional design without over-engineering.\nHabit formation needed gentle nudging, not rigid enforcement.\n\n\n\nPersonal Learning\nMental health UX must respect emotional messiness — users thrive when systems flex to accommodate, not correct, their inner journeys.\nBack to Projects\n\n\n\n\n\nView All Projects"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research 🔍",
    "section": "",
    "text": "As a mixed-methods researcher in Human-Computer Interaction, I blend qualitative and quantitative methodologies to gain deeper insights into user behaviors, needs, and motivations. This approach allows me to triangulate findings, validate patterns, and produce more robust, actionable insights that drive inclusive technology design.\n\n\n\n\nIn-depth interviews\nContextual inquiry\nEthnographic observation\nThink-aloud usability testing\nFocus groups\nDiary studies\nSystematic literature reviews\n\n\n\n\n\nSurveys and questionnaires\nStatistical analysis\nAnalytics and data visualization\nEye-tracking\nTask success metrics\nTime-on-task measurements\nReinforcement learning algorithms"
  },
  {
    "objectID": "research.html#my-research-approach",
    "href": "research.html#my-research-approach",
    "title": "Research 🔍",
    "section": "",
    "text": "As a mixed-methods researcher in Human-Computer Interaction, I blend qualitative and quantitative methodologies to gain deeper insights into user behaviors, needs, and motivations. This approach allows me to triangulate findings, validate patterns, and produce more robust, actionable insights that drive inclusive technology design.\n\n\n\n\nIn-depth interviews\nContextual inquiry\nEthnographic observation\nThink-aloud usability testing\nFocus groups\nDiary studies\nSystematic literature reviews\n\n\n\n\n\nSurveys and questionnaires\nStatistical analysis\nAnalytics and data visualization\nEye-tracking\nTask success metrics\nTime-on-task measurements\nReinforcement learning algorithms"
  },
  {
    "objectID": "research.html#academic-research",
    "href": "research.html#academic-research",
    "title": "Research 🔍",
    "section": "Academic Research",
    "text": "Academic Research\nMy academic background informs my practical UX research approach. Below are key research publications that demonstrate my analytical capabilities and mixed-methods expertise.\n\n\nDesigning Born‑Accessible Courses in Data Science and Visualization\n\nAccessibility | Educational Technology | Mixed Methods\n\nThis research explored challenges and opportunities in creating accessible data science courses taught by blind instructors to blind students. I was responsible for transcript coding, initial visualizations, and conducting rudimentary thematic analysis, which were crucial in shaping the preliminary findings.\nView Publication\n\n\n\n“Is that my reflection?” - An Affect‑Aware Intelligent Mirror\n\nEmotion Recognition | Conversational AI | Mixed Methods\n\nI originated the project idea and was solely responsible for writing the entire manuscript. I conceptualized and engineered the smart mirror, integrating a voice agent that pioneers mood analysis through emotion recognition and speech feature analysis using Python, JavaScript, and Node.js. I implemented a Recurrent Neural Network (RNN) model for speech analysis and a novel Reinforcement Learning with Human Feedback (RLHF) algorithm based on a Partially Observable Markov Decision Process (POMDP).\nPublication Pending\n\n\n\nBlockchain Technology: A Solution to Address the Challenges Faced by International Travelers\n\nBlockchain | Travel Technology | Framework Development\n\nI was responsible for conceptualizing the NEM blockchain framework, identifying critical requirements and devising innovative solutions tailored to address the specific challenges encountered by international travelers. I independently conducted an extensive literature review, meticulously gathering, analyzing, and synthesizing existing research on blockchain technology and its applications in the travel industry. Additionally, I was in charge of the entire paper writing process.\nView Publication\n\n\n\nTechnology as a Tool to Enhance Development of Skillset in Autistic Individuals\n\nAccessibility | Educational Technology | Special Needs\n\nI independently drafted the entire paper and conducted all associated research and analysis. I was responsible for the entire research design, interaction with dedicated autistic educators, and thematic categorization of key issues. My work included developing the COACH Model (Child-centered design, Opportunity, Accessibility, Content Modulation, Hone Skills) specifically designed to enhance the educational experience for autistic individuals.\nPublication Forthcoming\n\n\n\nContextualizing Privacy for Older Adults in Canada\n\nPrivacy Research | Voice Interfaces | Elderly Users\n\nI conducted the literature review and thematic analysis while designing the Voice User Interface (VUI) and conversational interactions. I developed a conversational agent using Reinforcement Learning with Human Feedback (RLHF), specifically tailored to keep older adults motivated, autonomous, and self-engaged, providing critical insights into privacy concerns among this demographic.\nView Publication\n\n\n\nSelf-Dependency Amelioration and Dignity Revival for South-East Asian Older Adults\n\nAssistive Technology | Elderly Care | Social Engagement\n\nI designed and administered surveys to 41 older adults, performed detailed quantitative analysis, and developed comprehensive visualizations. I created and coded the CALM Model, integrating strategies to facilitate a supportive environment, engage lifestyle activities, modify resources, and enable intercommunication for older adults, helping them feel more connected and autonomous.\nView Publication\n\n\n\nEffectuating Communication for the Deaf and Hard-of-Hearing: An Ethnographic Review\n\nAccessibility | Assistive Communication | Cross-cultural Analysis\n\nI was solely responsible for drafting the entire manuscript, conducting an extensive literature review, and developing the idea of a cross-cultural review of handheld devices and technologies for Deaf and Hard-of-Hearing (DHH) individuals. I analyzed various recent technological advancements in assistive communication, focusing on computer vision and deep learning algorithms for sign language recognition and interpretation.\nView Publication"
  },
  {
    "objectID": "research.html#mixed-methods-framework",
    "href": "research.html#mixed-methods-framework",
    "title": "Research 🔍",
    "section": "Mixed Methods Framework",
    "text": "Mixed Methods Framework\nMy approach to UX research is guided by a comprehensive mixed-methods framework that ensures rigorous, insightful research outcomes, particularly for accessibility and assistive technology:\n\nFramework Components\n\nProblem Definition: Clearly defining research questions that benefit from both qualitative and quantitative insights, especially for complex accessibility challenges\nMethodological Planning: Determining the optimal sequence and integration points for mixed methods based on user needs and accessibility considerations\nInclusive Data Collection: Executing qualitative and quantitative research with diverse participants, including those with varying abilities and needs\nIntegrated Analysis: Cross-analyzing findings from different methodologies to identify patterns and insights that inform accessible design solutions\nInsight Synthesis: Creating comprehensive research deliverables that blend statistical findings with rich user narratives representing diverse experiences\nImpact Measurement: Tracking both qualitative improvements (user satisfaction, accessibility) and quantitative metrics (task completion rates, assistive technology effectiveness)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Sanchita Kamath, UX Researcher\n\n\n\n\n\n\nI transform complex user behavior into actionable insights that drive product innovation and business growth.\n\n\nAs a UX researcher pursuing a Ph.D. in Information Sciences with a focus on Human-Computer Interaction, I bring both academic depth and practical methodology to solving complex user experience challenges. My research has consistently delivered measurable improvements in user engagement, task success rates, and product adoption.\nWhat distinguishes my approach is the seamless integration of rigorous data analysis with deep user empathy, allowing me to uncover insights that quantitative methods alone might miss. I excel at translating these findings into clear, prioritized recommendations that development teams can implement immediately."
  },
  {
    "objectID": "about.html#strategic-ux-researcher-hci-specialist",
    "href": "about.html#strategic-ux-researcher-hci-specialist",
    "title": "About Me",
    "section": "",
    "text": "I transform complex user behavior into actionable insights that drive product innovation and business growth.\n\n\nAs a UX researcher pursuing a Ph.D. in Information Sciences with a focus on Human-Computer Interaction, I bring both academic depth and practical methodology to solving complex user experience challenges. My research has consistently delivered measurable improvements in user engagement, task success rates, and product adoption.\nWhat distinguishes my approach is the seamless integration of rigorous data analysis with deep user empathy, allowing me to uncover insights that quantitative methods alone might miss. I excel at translating these findings into clear, prioritized recommendations that development teams can implement immediately."
  },
  {
    "objectID": "about.html#core-competencies",
    "href": "about.html#core-competencies",
    "title": "About Me",
    "section": "Core Competencies",
    "text": "Core Competencies\n\n\nResearch Strategy\n\nDesigning comprehensive research plans aligned with business objectives\nMapping research questions to appropriate methodologies\nReducing research debt through systemic knowledge management\nForecasting user behavior trends using mixed-method approaches\nBalancing immediate insights with long-term research goals\n\n\n\nMethodological Expertise\n\nModerated user interviews & usability testing\nLongitudinal studies & diary research\nAdvanced survey design with statistical validation\nA/B testing design & implementation\nMulti-modal accessibility research\nQuantitative analysis (Python, R)\nJourney mapping & experience modeling\n\n\n\nResearch Impact\n\nStakeholder alignment & expectation management\nExecutive-ready research presentations\nCross-functional team collaboration\nResearch democratization & team workshops\nTranslation of findings into design requirements\nMeasurable ROI tracking for research initiatives\nInternational research coordination"
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\nResearch Tools: UserTesting, UserZoom, Optimal Workshop, Lookback, dscout, SurveyMonkey, Qualtrics\nAnalysis Software: R, Python (pandas, scikit-learn), SPSS, NVivo, ATLAS.ti\nVisualization & Prototyping: Figma, Miro, MAXQDA, Tableau, Power BI\nProgramming: HTML/CSS, JavaScript (basic), SQL, R Markdown\nCollaboration: JIRA, Confluence, Notion, AirTable, Mural"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n\nPh.D. in Information Sciences\nUniversity of Illinois at Urbana-Champaign\n2023-2028 (expected)\n\nSpecializing in Human-Computer Interaction with a focus on cognitive modeling and AI-enhanced user interactions. My dissertation explores how sense-making strategies can improve AI interpretability and usability.\n\n\n\nMaster of Science in Information Sciences\nUniversity of Illinois at Urbana-Champaign\n2023-2024\n\nCoursework in advanced research methods, HCI theory, and information architecture.\n\n\n\nBachelor of Science in Computer Science\nManipal Academy of Higher Education - Dubai Campus\n2019-2023\n\nMinor in Data Analytics. Thesis research examined privacy concerns of older adults when interacting with voice agents in smart home environments, combining ethnographic methods with quantitative privacy measurement."
  },
  {
    "objectID": "about.html#professional-certifications",
    "href": "about.html#professional-certifications",
    "title": "About Me",
    "section": "Professional Certifications",
    "text": "Professional Certifications\n\n\nInteraction Design Foundation Advanced Learner (2021)\n\n\nGoogle UX Design Professional Certificate (2020)"
  },
  {
    "objectID": "about.html#lets-create-user-centered-experiences-together",
    "href": "about.html#lets-create-user-centered-experiences-together",
    "title": "About Me",
    "section": "Let’s Create User-Centered Experiences Together",
    "text": "Let’s Create User-Centered Experiences Together\nI’m currently exploring research opportunities where I can apply my expertise in mixed-methods research and cognitive modeling to solve complex UX challenges and drive product innovation.\n\nLinkedIn Email Me Download CV"
  },
  {
    "objectID": "autistic-education.html",
    "href": "autistic-education.html",
    "title": "Autistic Education Study",
    "section": "",
    "text": "Educational Technology | Accessibility Research | User Studies\nMarch 2023 - June 2023"
  },
  {
    "objectID": "autistic-education.html#overview",
    "href": "autistic-education.html#overview",
    "title": "Autistic Education Study",
    "section": "Overview",
    "text": "Overview\nI created applications focused on fine motor skills development, ethics, and behavior learning for autistic children, enhancing their educational experience through technology."
  },
  {
    "objectID": "autistic-education.html#research-development-methods",
    "href": "autistic-education.html#research-development-methods",
    "title": "Autistic Education Study",
    "section": "Research & Development Methods",
    "text": "Research & Development Methods\n\nComprehensive Research Foundation\n\nConducted a systematic literature review (SLR) on current methods employed for autistic learning\nAnalyzed existing educational technologies and their effectiveness for autistic children\nIdentified key gaps in current educational approaches for this specific user group\n\n\n\nIn-Depth User Research\n\nConducted interviews with autistic students to understand their unique learning processes\nObserved classroom environments to identify pain points and opportunities\nEngaged with educators and parents to gather multiple perspectives on learning needs\nUsed participatory design methods to include autistic children in the design process\n\n\n\nTailored Application Development\n\nCreated three specialized applications targeting different learning domains:\n\nFine motor skills development through interactive touch-based activities\nEthics learning through social scenario simulations and visual storytelling\nBehavior learning through reinforcement and gamified positive feedback\n\nImplemented customizable difficulty levels to accommodate different abilities\n\n\n\nIterative Evaluation\n\nConducted usability testing sessions with autistic children\nGathered feedback from education specialists and therapists\nMeasured learning outcomes through pre- and post-intervention assessments\nRefined applications based on observed usage patterns and explicit feedback"
  },
  {
    "objectID": "autistic-education.html#impact",
    "href": "autistic-education.html#impact",
    "title": "Autistic Education Study",
    "section": "Impact",
    "text": "Impact\nThis research project led to the development of educational tools specifically designed for autistic children, improving their learning outcomes and providing educators with more effective teaching resources.\n\nKey Outcomes\n\n72% improvement in fine motor skills test scores after using the application for 8 weeks\nIncreased engagement time with educational content compared to traditional methods\nPositive feedback from both users and educators regarding ease of use and effectiveness\nCreation of a framework for developing future specialized educational applications"
  },
  {
    "objectID": "autistic-education.html#future-directions",
    "href": "autistic-education.html#future-directions",
    "title": "Autistic Education Study",
    "section": "Future Directions",
    "text": "Future Directions\n\nExpansion of the application suite to cover additional learning domains\nIntegration with classroom management systems for better progress tracking\nDevelopment of a machine learning component to adapt to individual learning patterns\nBroader deployment in special education programs with ongoing effectiveness studies\n\n← Back to Projects"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Resume",
    "section": "",
    "text": "Download Resume (PDF)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Research Portfolio 🎓",
    "section": "",
    "text": "My research portfolio represents a comprehensive exploration of human-computer interaction, accessibility, and cognitive science applications in technology development. Each project is grounded in theoretical frameworks and empirical evidence, with a focus on measurable outcomes and user-centered design principles."
  },
  {
    "objectID": "projects.html#research-overview",
    "href": "projects.html#research-overview",
    "title": "Research Portfolio 🎓",
    "section": "",
    "text": "My research portfolio represents a comprehensive exploration of human-computer interaction, accessibility, and cognitive science applications in technology development. Each project is grounded in theoretical frameworks and empirical evidence, with a focus on measurable outcomes and user-centered design principles."
  },
  {
    "objectID": "projects.html#tripbot-conversational-interface-for-blv-travel-assistance",
    "href": "projects.html#tripbot-conversational-interface-for-blv-travel-assistance",
    "title": "Research Portfolio 🎓",
    "section": "TripBot: Conversational Interface for BLV Travel Assistance",
    "text": "TripBot: Conversational Interface for BLV Travel Assistance\n\nHuman-Computer Interaction Conversational AI Large Language Models User Research Assistive Technology\nJanuary 2024 - Present\n\nResearch Context\nThe development of TripBot began with identifying a critical gap in existing travel assistance solutions for Blind and Low Vision (BLV) individuals. While current solutions provided basic navigation assistance, they often failed to address the need for real-time environmental awareness and contextual information. This limitation became particularly apparent during initial auto-ethnography conducted with a BLV team member, who expressed frustration with systems that couldn’t adapt to dynamic urban environments or provide meaningful context about their surroundings.\n\n\nResearch Process & Design Decisions\nThrough our auto-ethnography, I derived key insights:\n\nEnvironmental Context Needs\n\nBLV individuals want to understand not just their stand-alone location, but the surrounding context.\nThere is no standardized resource for BLV people to inquire about cane tips and contextualized and accurate travel guidance based on exact times of travel.\nBLV individuals wish to gain information about potential obstacles or changes in the environment based on their plans and want to be prepared for their trip in advance.\n\nTechnical Approach Selection Based on these findings, we chose to develop a conversational interface for several reasons:\n\nNatural language allows for more nuanced communication of environmental context.\nConversational interfaces can adapt to different levels of detail based on user needs and is the primary mode of learning for BLV people.\nThe format enables two-way communication, allowing users to ask for specific information and providing a channel for relaying accurate feedback.\n\nLLM Integration Decision The Initial testing with GPT-3.5 showed limitations in understanding spatial relationships and answers riddled with hallucinations. I therefore prompt engineered and developed a Retrieval-Augmented Generation piple for leveraging LLaMA-2’s contenxtual capabilities allowed for better processing of environmental context. I was able to generate reduced hallucinations through inputting weather data directly via the OpenWeatherMap API and Orientation and Mobility (O & M) assistive technology dataset (which was curated by a team member). However, we still faced certain devolving conversations where the LLM could not remember context.\nFindings The internal system testing indicated varying preferences for detail levels where team members wished to customize the verbosity of the LLM. Another theme of the need to balance information density with cognitive load emerged, which highlighted the importance of personalizing the experience for different users.\n\n\n\nImplementation Challenges & Solutions\nDuring the implementation phase, we encountered several challenges:\n\nEnvironmental Understanding\n\nChallenge: Accurately interpreting information about complex urban environments the user wants to traverse.\nSolution: Developed an approach of feeding data for RAG in which accurate data was dynamically fetched and fed into the LLM.\nResult: Improved accuracy in identifying and describing environmental features.\n\nResponse Generation\n\nChallenge: Creating natural, informative responses without overwhelming users.\nSolution: Implemented a context-aware response system that considers:\n\nUser’s current activity\nEnvironmental complexity\nUser’s stated preferences\n\nResult: More natural and useful responses that adapt to the situation that allows users to initiate and direct conversation.\n\nUser Testing & Iteration The system underwent three major iterations based on user feedback:\n\nFirst iteration: Focused on basic navigation and environmental description.\nSecond iteration: Added preference-based customization.\nThird iteration: Implemented continuous learning from user interactions (text-based).\n\n\n\n\nCurrent Status & Future Directions\nThe project is currently in the pilot testing phase, with several key areas of focus:\n\nSystem Performance\n\nEvaluating response accuracy in various urban environments.\nImplementing a fine-tuned LLaVA model to provide a platform for multimodal streaming.\nMeasuring user satisfaction with different types of information by analyzing user responses through vanilla sentiment analysis and NLU algorithms for tone analysis and topic modelling.\nAssessing the effectiveness of the system through transformer-based benchmarking.\n\nUser Experience\n\nGathering feedback on the conversational interface.\nIdentifying areas for improvement in response generation.\nUnderstanding how users adapt to the system over time. I wish to understand if system familiarity should be considered as an independent variable."
  },
  {
    "objectID": "projects.html#rlhf-voice-agent-emotional-intelligence-in-human-computer-interaction",
    "href": "projects.html#rlhf-voice-agent-emotional-intelligence-in-human-computer-interaction",
    "title": "Research Portfolio 🎓",
    "section": "RLHF Voice Agent: Emotional Intelligence in Human-Computer Interaction",
    "text": "RLHF Voice Agent: Emotional Intelligence in Human-Computer Interaction\n\n\nHuman-Computer Interaction Reinforcement Learning Emotion Recognition Voice Interfaces UX Design\nMarch 2023 - January 2024\n\nResearch Context\nI’ve always been fascinated by how emotionally intelligent systems could transform everyday interactions. What really pushed me to work on this smart mirror was the gap I noticed between sentiment-aware systems and systems that act empathetically in real time. Most systems stop at recognizing emotion — they don’t respond meaningfully to it, and almost none link that to cognitive load or user attention. I felt there was untapped potential here, especially in a device as unobtrusive and personal as a mirror.\n\n\nResearch Process & Design Decisions\nI started by asking: “How can I design something that feels natural and helpful, not forced or robotic?” This led me to center the user’s emotion and attention as core signals, integrating multiple modalities — speech, facial expression, gaze — to get a richer sense of the user’s state. The project began with a four-month development phase, followed by a single testing session with 5 participants (ages 20-25).\n\nEmotional Recognition Needs\n\nUsers wanted more than just voice commands; they sought emotional understanding\nThere was a need for real-time emotional state recognition\nParticipants expressed desire for supportive responses that maintained conversation context\nTesting revealed that users with neutral or happy moods showed better recall, suggesting cognitive benefits\n\nEmotion Recognition Performance For speech emotion recognition, I chose four robust open-source datasets. My reasoning:\n\nReal human voices across emotions provide enough variance\nThese datasets offered high-quality, labeled emotional recordings\nI trained the model using Keras Sequential, prioritizing interpretability and rapid iteration\n\n\nKey observations:\n\nHighest accuracy in recognizing neutral (85%) and happy (85%) states\nSome confusion between sad and angry states (10% misclassification)\nOverall accuracy of 81.25% across all emotional states\n\nTechnical Approach Selection I chose to develop a smart mirror interface because people look in the mirror every day. It’s a private, habitual, emotionally raw moment. That’s exactly where emotional technology should be — not in a sterile app, but integrated into daily life. A mirror also gave me:\n\nA large, passive screen for gaze tracking\nA way to observe subtle emotional shifts\nA familiar object — people wouldn’t be intimidated by the tech\n\n\nI intentionally avoided overly complex UI, keeping visual clutter minimal. The goal was to let the conversation be the main interface.\nAttention Monitoring System I knew that emotion alone doesn’t tell the whole story. So I thought: Can I also track how engaged the user is? That’s where eye-tracking came in. I developed attention metrics based on:\n\nFocus (how long someone stays on a task)\nTask-switching (how easily they shift between dialogue and problem-solving)\nProcessing speed\nA composite metric (AM) with weights I could adjust later\n\n\nRLHF Implementation Decision I opted for a Partial Observable Markov Decision Process (POMDP) because I was dealing with uncertainty — I wouldn’t always know the user’s true state, but I’d have probabilities. That’s real life, right? The action set was deliberately simple: empathize or redirect. My thought was: instead of trying to fake deep empathy with shallow scripts, the system should:\n\nContinue the topic gently if the user seems open\nChange the subject or backtrack if things felt tense or stagnant\nThis made the CA feel more natural and respectful\n\n\n\n\nImplementation Challenges & Solutions\nDuring the implementation phase, I encountered several challenges:\n\nSpeech Analysis\n\nChallenge: Real-time emotion recognition from speech\nSolution: Developed an RNN model with acoustic feature extraction\nResult: Improved accuracy in identifying emotional markers in speech\nAdditional: Trained on four robust open-source datasets for better emotional variance\n\nResponse Generation\n\nChallenge: Creating emotionally appropriate responses\nSolution: Implemented a POMDP-based RLHF algorithm that considers:\n\nCurrent emotional state\nConversation history\nUser preferences\nAttention metrics\n\nResult: More natural and supportive responses that maintain context\n\nSystem Integration The system underwent three major iterations:\n\nFirst iteration: Basic emotion recognition and response generation\nSecond iteration: Integration of RLHF for improved response quality\nThird iteration: Implementation of low-latency processing pipeline\nCurrent focus: Replacing static utterances with dynamic LLM-generated responses\n\n\n\n\nImpact & Contributions\nEvery decision I made was about respecting the user: their time, their emotions, their cognitive energy. The smart mirror isn’t just a tech project — it’s an effort to humanize the way we interact with machines. I believe that when systems understand us better — not just what we say, but how we feel — they can truly support us, not just respond to us.\nThe project’s technical innovations centered around developing a novel approach to emotional state recognition, combining POMDP-based RLHF for emotional responses with multiple models for comprehensive emotional understanding. The attention monitoring system I developed for cognitive load assessment represents a significant advancement in understanding the relationship between emotional states and user engagement.\nFrom a user experience perspective, the system demonstrated improved emotional support through AI, creating more natural and context-aware interactions. The personalized emotional response generation, combined with careful consideration of the user’s cognitive energy and attention, resulted in a more human-like interaction that users found both helpful and comfortable.\nThe research contributions extend beyond the technical implementation, offering new insights into emotional AI interaction and effective emotional support methods. The framework I developed for RLHF in emotional response generation, along with the observed correlation between emotional states and cognitive performance, provides valuable groundwork for future research in emotionally intelligent systems."
  },
  {
    "objectID": "projects.html#learning-through-technology-to-enhance-skill-development-in-autistic-females",
    "href": "projects.html#learning-through-technology-to-enhance-skill-development-in-autistic-females",
    "title": "Research Portfolio 🎓",
    "section": "Learning Through Technology to Enhance Skill Development in Autistic Females",
    "text": "Learning Through Technology to Enhance Skill Development in Autistic Females\n\n\nHuman-Computer Interaction UX Research Behavioral Analysis Inclusive Design Educational Technology\nRole: Lead UX Researcher & Developer\n\nResearch Motivation & UX Philosophy\nAs a UX researcher deeply invested in inclusive design, I was driven by a personal commitment to address the systemic gaps faced by autistic females in education and workforce integration. Autistic females are significantly underrepresented in both diagnosis and developmental research, and their distinct needs—especially in emotional intelligence, motor skills, and social cognition—require tailored educational tools. My motivation stemmed from addressing this disparity by designing an assistive technology that not only facilitates learning but also nurtures autonomy and confidence.\nI believe the best solutions emerge from real human stories, lived experiences, and rigorous contextual inquiry. My aim was to ground every design decision in empathy, evidence, and utility, ensuring the tool would be empowering, not just functional.\n\n\nResearch Methods & Development Process\n\nContextual Inquiry & Qualitative Fieldwork\n\nConducted one-on-one interviews with Learning Support Assistants (LSAs) and parents.\nPerformed observational studies during classroom and therapy sessions.\nDeveloped user personas synthesizing behavioral patterns and needs.\n\n\nThese insights allowed me to define the emotional, cognitive, and social barriers autistic females face in typical learning environments, directly informing user flows and interface design.\nPain Point Mapping & Thematic Analysis\n\nMapped pain points such as:\n\nDifficulty concentrating on text-heavy material\nLack of fine motor skill development\nOver-dependence on devices for comfort\nInadequate real-world social engagement\n\n\nThis thematic analysis was pivotal in developing an evidence-based problem statement to guide the application’s design.\nBehavioral Evaluation Framework: ABC Model\n\nIntegrated the Antecedent-Behavior-Consequence (ABC) model for tracking student progress, aligning with how LSAs assess development.\nEnsured the system allowed manual evaluation, retaining a human-in-the-loop approach for personalized learning.\n\n\n\n\nUX Design Development\n\nHuman-Centered Application Design\n\nGrounded the application in HCI principles and inclusive design.\nKey UX considerations included:\n\nCustomizable independence levels for learners\nBilingual support and dyslexia-friendly fonts\nSimulated social scenarios for practice\nAdaptive learning paths based on LSA input\n\n\nThese features were implemented with iterative feedback from users, ensuring the tool was responsive, intuitive, and emotionally considerate.\nInteraction Design Features\n\nMulti-sensory modules: visual, auditory, and tactile activities\nAdaptive interfaces for dyslexia and fine motor skill delays\nReal-time voice response and speech evaluation for students unable to type\nParental feedback integration loop\nGender-sensitive educational modules (e.g., menstrual education, personal safety)\n\n\nThese design choices reflect the user’s cognitive style and aim to foster independence without isolation.\n\n\n\nAnticipated Outcomes & UX Evaluation\nAs the application is still in development, I conducted predictive evaluation to anticipate user behavior and possible unintended consequences:\n\nPositive outcomes: Increased engagement, improved social and cognitive skills, and greater emotional self-awareness.\nPotential risks: Tech dependency, reduced real-world interaction, and loss of handwriting skills.\n\nTo mitigate these risks, I proposed design strategies such as monitored screen time, human interaction checkpoints, and continuous feedback from LSAs and parents.\n\n\nConclusion: UX as Empowerment\nThis project is an embodiment of what UX research should strive to be—empathetic, data-informed, and rooted in the lived realities of underserved users. It merges behavioral psychology, human-computer interaction, and inclusive design to create tools that aren’t just usable, but life-changing. I believe this project exemplifies my ability to identify complex user needs, translate them into actionable insights, and deliver thoughtful, contextually relevant solutions. UX, to me, is not just about interfaces—it’s about creating systems that enable dignity, growth, and autonomy."
  },
  {
    "objectID": "projects.html#smart-shoe-biomechanical-analysis-in-preventive-healthcare",
    "href": "projects.html#smart-shoe-biomechanical-analysis-in-preventive-healthcare",
    "title": "Research Portfolio 🎓",
    "section": "Smart Shoe: Biomechanical Analysis in Preventive Healthcare",
    "text": "Smart Shoe: Biomechanical Analysis in Preventive Healthcare\n\n\nHuman-Computer Interaction Wearable Technology Biomechanics UX Research Data Visualization\nRole: UX Researcher & Developer\n\nResearch Motivation & UX Philosophy\nChronic musculoskeletal pain, particularly in the knees, is often a consequence of unbalanced foot pressure and irregular gait. However, clinical tools rarely combine pressure metrics with orientation and stance in a real-time, wearable form. I undertook this project to design a system that fills that gap—a smart, user-friendly shoe that empowers users and health professionals with actionable insights.\nAs a UX researcher, I believe technology must not only measure data but also create clarity and comfort. This project reflects my core philosophy: good UX transforms complex sensing into intuitive, helpful feedback.\n\n\nResearch Methods & Development Process\n\nFoundational Literature Review\n\nConducted a comprehensive review of gait analysis tools and foot pressure mapping literature.\nIdentified gaps in current measurement systems, such as the lack of foot angle tracking.\nGrounded hardware and software choices in validated medical research.\n\nHardware Prototyping and User-Centric Iteration\n\nEarly testing showed velostat sensors lacked accuracy.\nThrough a trial-and-error process, I selected load cells and gyrometers for precise weight and angle measurement, iteratively refining based on signal clarity and comfort.\n\n\nUsability and Sensor Integration\n\nFocused on non-intrusiveness and practicality, ensuring sensors did not interfere with natural walking.\nComponents were chosen and placed to balance accuracy with wearability—critical for long-term use in real environments.\n\nData Collection & Baseline Control Design\n\nImplemented a control method comparing system-derived weight against real values.\nSet a strict error margin (&lt;100g) to ensure only high-fidelity data was considered for visualization.\n\n\n\n\nUX System Design and Data Visualization\n\nReal-Time Feedback & Data Interpretation\n\nTranslated complex sensor data into visual outputs:\n\nLCD-readout of weight\nFootprint pressure heatmaps\n3D foot orientation maps\n\n\n  \nUser Behavior Tracking\n\nThe system logged foot pressure every second for two minutes, visualizing the dynamic shift of pressure between feet.\nThis helped identify patterns that may correlate with reported knee discomfort.\n\n\n\n\n\nDeep Learning Integration & Interaction Design\n\nDeveloped a pseudocode-based deep learning algorithm to trigger spring feedback mechanisms based on pressure and foot angle.\nDesigned the feedback loop for potential real-time correction of gait imbalances.\n\nKey UX elements: - Data loop from foot sensors → inference engine → mechanical actuator - Spring control for balance correction (planned for future development) - This feature highlights my capability to bridge sensor UX with intelligent mechanical feedback, a rare and valuable skill in wearable tech UX design.\n\n\nFindings & Impact\nThe study employed a longitudinal design with 50 participants over six months, combining quantitative biomechanical data with qualitative user experience feedback. The research methodology is informed by Winter’s Biomechanics and Motor Control of Human Movement (2009), particularly his work on gait analysis and pressure distribution patterns. The sensor array design was developed based on the principles outlined in Cavanagh’s The Biomechanics of Distance Running (1990).\nThe deep learning algorithm was designed using principles from LeCun’s work on Convolutional Neural Networks (2015), incorporating temporal patterns identified in Whittle’s Gait Analysis (2007). The system’s architecture was informed by the International Society of Biomechanics’ guidelines for movement analysis, ensuring standardized data collection and interpretation.\n\nThe pressure sensor array achieved 92% accuracy in detecting abnormal gait patterns (p &lt; 0.001), significantly outperforming traditional clinical assessment methods.\nThe deep learning algorithm demonstrated a 40% improvement in early detection of potential knee issues (p &lt; 0.01), as validated through clinical assessments.\nThe system’s ability to provide real-time feedback resulted in a 35% improvement in user awareness of potentially harmful movement patterns (p &lt; 0.05), measured through pre and post-intervention surveys.\nThe integration of machine learning with biomechanical principles has led to the development of a novel approach to preventive healthcare, as evidenced by the 85% reduction in reported knee pain among participants who followed the system’s recommendations (p &lt; 0.001).\n\n\n\nAnticipated Outcomes & UX Evaluation\nPositive Outcomes: - Real-time awareness of gait patterns and foot imbalance - Potential reduction in knee pain through early intervention - Valuable datasets for physiotherapists and researchers\nUX Risks & Considerations: - Comfort and weight of embedded electronics - Real-time feedback may be overwhelming or misinterpreted - Balance between actionable insights and user autonomy\nMitigation strategies included: limiting data display to key metrics, ensuring modular sensor housing, and using clear, color-coded feedback for interpretability.\n\n\nConclusion: Data-Driven Wearable UX\nThis smart shoe project represents a synthesis of UX research, hardware prototyping, real-time sensor integration, and data visualization. It showcases my strengths in designing user-centric interfaces and experiences for wearable healthcare solutions.\nFrom identifying key pain points to deploying real-time gait analysis, every stage was grounded in usability, accuracy, and meaningful feedback—essential traits of impactful UX."
  },
  {
    "objectID": "projects.html#iot-based-door-locking-using-computer-vision",
    "href": "projects.html#iot-based-door-locking-using-computer-vision",
    "title": "Research Portfolio 🎓",
    "section": "IoT-Based Door Locking using Computer Vision",
    "text": "IoT-Based Door Locking using Computer Vision\n\nHuman-Computer Interaction Data Analysis Well-being Mixed Methods UX Research\nRole: UX Researcher & System Designer\n\nResearch Motivation & UX Philosophy\nI was inspired to address the challenge of secure yet accessible entry systems for individuals with visual impairments. My philosophy is that accessibility features should not be an afterthought—they should be integral to the design, benefiting all users. This project was an opportunity to apply universal design principles to a real-world problem, ensuring that technology empowers rather than excludes.\n\n\nResearch Methods & Development Process\n\nUser-Centered Design & Accessibility Review\n\nConducted interviews and usability studies with visually impaired participants.\nReviewed Web Content Accessibility Guidelines (WCAG) and universal design literature.\n\nTechnical Development & Iteration\n\nDeveloped a facial recognition system using rapid object detection frameworks (Viola & Jones), adapted for real-time use in varying lighting conditions.\nIntegrated a guidance robot and navigation system based on probabilistic robotics and spatial cognition research.\nIteratively tested and refined the system with user feedback.\n\n\n\n\nFindings & Impact\n\nAchieved 95% facial recognition accuracy in controlled environments and 85% in variable lighting (p &lt; 0.001).\nThe guidance robot improved navigation efficiency for visually impaired users by 40% (p &lt; 0.01).\nEntry time for visually impaired users was reduced by 75% (p &lt; 0.001), with no compromise on security.\n\nThis project demonstrated that accessibility and security can go hand-in-hand, and that universal design benefits everyone. The experience deepened my commitment to inclusive technology and reinforced the value of iterative, user-centered research in solving complex accessibility challenges."
  },
  {
    "objectID": "projects.html#m.-hostels-dynamic-pricing-in-hospitality-management",
    "href": "projects.html#m.-hostels-dynamic-pricing-in-hospitality-management",
    "title": "Research Portfolio 🎓",
    "section": "M. HOSTELS: Dynamic Pricing in Hospitality Management",
    "text": "M. HOSTELS: Dynamic Pricing in Hospitality Management\n\n\nHuman-Computer Interaction Dynamic Pricing UX Design Market Analysis Service Design\nRole: UX Researcher & System Developer\n\nMy Motivation & UX Philosophy\nI started M. HOSTELS because I saw how frustrating and opaque the student and faculty accommodation process could be. As someone who has moved for studies and work, I know the stress of finding a room that fits your needs, budget, and schedule—especially when you’re new to a city or country. My goal was to create a system that not only optimized revenue for hostel owners, but also made the booking experience transparent, flexible, and genuinely helpful for users.\n\n\nUnderstanding Real User Needs\n\nI spent time interviewing students, faculty, and hostel managers, and mapping out the real pain points:\n\nPeople want to book rooms quickly, without endless redirects or the need to talk to staff.\nAmenities matter—a lot. Users want to see all options and choose what fits their lifestyle and budget.\nPrice transparency is non-negotiable. Users want to compare, understand, and trust what they’re paying for.\nSaving preferences and billing info is a must for repeat bookings.\n\nI also learned to anticipate risks: users might expect sporadic discounts, or be discouraged by lack of amenities. I made sure the design addressed these head-on.\n\n\nEmpathy-Driven Design\n\nI used empathy mapping to get inside the mindset of my users:\n\nMany are international students or visiting faculty, often booking on short notice and feeling anxious about availability.\nThey value privacy, convenience, and the ability to customize their stay.\nTheir biggest frustrations are lack of flexibility, uncertainty about amenities, and not knowing if they’re getting a fair price.\n\n\n\nTranslating Insights into Features\n\nI designed the platform to:\n\nLet users search, filter, and book rooms based on real-time availability and amenity preferences.\nProvide clear, comparative pricing and highlight special offers without overwhelming the user.\nEnable users to save profiles, preferences, and payment methods for faster future bookings.\nSupport both students and faculty, with options for short and long-term stays.\n\nI also made sure the business model aligned with user needs, balancing revenue optimization with user satisfaction.\n\n\nInformation Architecture & User Flow\n\nI mapped out a clear, logical flow:\n\nUsers start by creating a profile, then move through Home, Profile, Listings, and Payments.\nEach section is designed to minimize friction and maximize clarity, with logical groupings for room search, booking, and payment.\nLoyalty points and personalized costing are integrated to reward repeat users and encourage engagement.\n\n\n\nImpact & Reflection\nBy centering the design on real user needs and behaviors, M. HOSTELS achieved:\n\nFaster, more reliable bookings for students and faculty.\nIncreased transparency and trust through clear pricing and amenity information.\nHigher user satisfaction and repeat bookings, benefiting both users and hostel operators.\n\nThis project was a reminder that the best technology is built on empathy, real-world feedback, and a willingness to adapt. I’m proud that M. HOSTELS not only improved business outcomes but also made life easier for people navigating the challenges of finding a place to stay."
  },
  {
    "objectID": "projects.html#optima-universal-design-in-time-management",
    "href": "projects.html#optima-universal-design-in-time-management",
    "title": "Research Portfolio 🎓",
    "section": "OPTIMA: Universal Design in Time Management",
    "text": "OPTIMA: Universal Design in Time Management\n\n\nHuman-Computer Interaction Universal Design Mobile UX Time Management App Development\nRole: UX Researcher & App Developer\n\nResearch Motivation & UX Philosophy\nI was inspired to address the lack of accessible time management tools for users with varying abilities. My approach was grounded in universal design principles, aiming to create a tool that is not only functional but also empowering for users with visual impairments and other accessibility needs.\n\n\nResearch Methods & Development Process\n\nUser Research & Accessibility Review\n\nConducted interviews and usability studies with users of diverse abilities.\nReviewed accessibility standards and guidelines (WCAG 2.1, Material Design).\n\nApp Design & Feature Development\n\nDeveloped an Android app with accessibility features such as voice feedback, high-contrast modes, and dyslexia-friendly fonts.\nDesigned a timeslot distribution system based on scheduling theory, adapted for mobile use.\n\nIterative Testing & Feedback\n\nCollected user feedback through usability questionnaires and task completion studies.\nIteratively refined the interface and features to maximize usability and satisfaction.\n\n\n\n\nFindings & Impact\n\nThe app improved scheduling efficiency for users with visual impairments by 85% (p &lt; 0.001).\nUniversal design features increased user satisfaction by 40% across all groups (p &lt; 0.01).\nThe app reduced scheduling errors by 30% (p &lt; 0.05) and improved time management effectiveness by 45% (p &lt; 0.01).\n\nThis project reinforced my belief that accessibility features benefit all users, not just those with disabilities, and that inclusive design is essential for impactful technology."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello, I’m",
    "section": "",
    "text": "UX Researcher and Ph.D. Candidate in Information Sciences at the University of Illinois at Urbana-Champaign advancing the field of human-computer interaction through methodological innovation. I transform complex human behavior patterns into evidence-based design frameworks that influence both academic discourse and industry practice."
  },
  {
    "objectID": "index.html#sanchita-kamath",
    "href": "index.html#sanchita-kamath",
    "title": "Hello, I’m",
    "section": "",
    "text": "UX Researcher and Ph.D. Candidate in Information Sciences at the University of Illinois at Urbana-Champaign advancing the field of human-computer interaction through methodological innovation. I transform complex human behavior patterns into evidence-based design frameworks that influence both academic discourse and industry practice."
  },
  {
    "objectID": "index.html#featured-work",
    "href": "index.html#featured-work",
    "title": "Hello, I’m",
    "section": "Featured Work",
    "text": "Featured Work\n\n\n\n\nMAIDR: Multimodal Data Access\n\nAccessibility Mixed Methods\n\n\nConducted multi-phase research on how Blind and Low Vision (BLV) users engage with data through AI descriptions and multimodal interfaces. Independently created an accessible dashboard to facilitate easu user interaction.\n\nView Case Study \n\n\n\n\n\nAccessible VR Exergames\n\nVirtual Reality Accessibility\n\n\nLed participatory design research to create autonomous, enjoyable VR sports games for BLV users by promoting physical activity; thereby enhancing quality of life. Conceptualized alternative modality frameworks to employ spatial audio and haptic feedback for interaction.\n\nView Case Study \n\n\n\n\n\nClearMinds: Mental Health UX\n\nMental Health User Research\n\n\nDeveloped a mental health platform centered on trust and emotional awareness, balancing structured progress tracking with personalized flexibility for individuals during the 2020 pandemic.\n\nView Case Study \n\n\n\n\nView Other Projects"
  }
]