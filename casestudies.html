<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Case Studies ‚Äì Sanchita Kamath | UX Researcher</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./img/favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-905433ffa22618bb005779f2b23c5ce0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;500;600;700&amp;family=Roboto:wght@300;400;500;700&amp;family=Open+Sans:wght@400;500;600;700&amp;display=swap" rel="stylesheet">


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./img/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Sanchita Kamath | UX Researcher</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./projects.html"> 
<span class="menu-text">Work üöÄ</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./research.html"> 
<span class="menu-text">Research üîç</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./writing.html"> 
<span class="menu-text">Writings üìì</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./cv.html"> 
<span class="menu-text">Resume</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/sanchitakamath/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:mail.kamath.sanchita@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/sk-143381"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Case Studies</h1>
<p class="subtitle lead">Featured Research &amp; Design Projects</p>
</div>



<div class="quarto-title-meta column-page">

    
  
    
  </div>
  


</header>


<style>
.article-content {
  display: none;
  padding: 20px;
  margin-top: 15px;
  border-top: 1px solid rgba(97, 114, 227, 0.2);
  border-radius: 0 0 12px 12px;
  background-color: rgba(97, 114, 227, 0.02);
  transition: all 0.5s ease-in-out;
}

.read-more-btn {
  color: #6172e3;
  cursor: pointer;
  display: inline-flex;
  align-items: center;
  margin-top: 15px;
  padding: 8px 0;
  font-weight: 500;
  font-family: 'Open Sans', sans-serif;
  transition: all 0.3s ease;
}

.read-more-btn:hover {
  color: #4a56c2;
  transform: translateY(-2px);
}

.read-more-btn i {
  margin-left: 5px;
  transition: transform 0.3s ease;
}

.read-more-btn:hover i {
  transform: translateY(2px);
}

.article-card {
  padding: 25px;
  border-radius: 12px;
  box-shadow: 0 5px 15px rgba(0, 0, 0, 0.05);
  transition: all 0.3s ease;
  background-color: white;
  height: 100%;
  display: flex;
  flex-direction: column;
}

.article-card:hover {
  transform: translateY(-8px);
  box-shadow: 0 10px 25px rgba(0, 0, 0, 0.08);
}

.article-card h2 {
  font-family: 'Roboto', sans-serif;
  font-size: 1.5rem;
  margin-bottom: 15px;
  color: #1a202c;
  transition: color 0.3s ease;
}

.article-card:hover h2 {
  color: #6172e3;
}

.article-intro {
  color: #4a5568;
  line-height: 1.7;
  margin-bottom: 10px;
}

.article-tags {
  display: flex;
  flex-wrap: wrap;
  margin: 10px 0;
  gap: 8px;
}

.article-tag {
  background-color: #e6f2ff;
  color: #2c5282;
  padding: 4px 10px;
  border-radius: 50px;
  font-size: 0.8rem;
  font-weight: 500;
}

.highlight-banner {
  padding: 2rem;
  background: linear-gradient(135deg, rgba(97, 114, 227, 0.08), rgba(176, 106, 179, 0.08));
  border-radius: 12px;
  margin-bottom: 3rem;
  box-shadow: 0 5px 15px rgba(0, 0, 0, 0.05);
  border-left: 5px solid #6172e3;
  animation: fadeIn 1s ease-in-out;
}

.writings-intro {
  margin-bottom: 0;
  font-size: 1.1rem;
  line-height: 1.7;
}

.writings-questions {
  margin-top: 1.5rem;
  padding-left: 1.2rem;
}

.writings-questions li {
  margin-bottom: 0.7rem;
  position: relative;
}

.section-heading {
  font-family: 'Roboto', sans-serif;
  font-weight: 700;
  font-size: 2.2rem;
  margin-bottom: 2rem;
  color: #1a202c;
}

@keyframes fadeIn {
  from { opacity: 0; transform: translateY(20px); }
  to { opacity: 1; transform: translateY(0); }
}

@keyframes staggerFade {
  from { opacity: 0; transform: translateY(30px); }
  to { opacity: 1; transform: translateY(0); }
}

.article-card-wrapper {
  opacity: 0;
  animation: staggerFade 0.6s forwards;
}

.article-card-wrapper:nth-child(1) { animation-delay: 0.1s; }
.article-card-wrapper:nth-child(2) { animation-delay: 0.2s; }
.article-card-wrapper:nth-child(3) { animation-delay: 0.3s; }
.article-card-wrapper:nth-child(4) { animation-delay: 0.4s; }
.article-card-wrapper:nth-child(5) { animation-delay: 0.5s; }
.article-card-wrapper:nth-child(6) { animation-delay: 0.6s; }
</style>
<div class="case-studies-intro">
<p class="lead-text">
Below are detailed case studies that demonstrate my research approach, methodologies, and impact. Each case study highlights different aspects of my UX research expertise across various domains.
</p>
</div>
<div class="case-studies-grid">
<hr>
<section id="enhancing-data-accessibility-through-multimodal-ux-for-blv-users-maidr-project" class="level2">
<h2 class="anchored" data-anchor-id="enhancing-data-accessibility-through-multimodal-ux-for-blv-users-maidr-project"><span id="tripbot" title="Enhancing Data Accessibility through Multimodal UX for BLV Users">Enhancing Data Accessibility through Multimodal UX for BLV Users (MAIDR Project)</span></h2>
<div class="project-details">
<p><span class="article-tag">Research Ethics</span> <span class="article-tag">Conversational AI</span> <span class="article-tag">Accessibility</span> <span class="article-tag">Mixed Methods Research</span> <span class="article-tag">Data Visualization</span></p>
<p><em>January 2024 - Present</em></p>
<section id="project-context" class="level3">
<h3 class="anchored" data-anchor-id="project-context">Project Context</h3>
<p>When I joined the MAIDR team, a pressing research gap was immediately visible:</p>
<p>While large language models (LLMs) could generate text descriptions of complex data visualizations, there was <strong>no structured understanding</strong> of <strong>how Blind and Low Vision (BLV) users would interact with, trust, customize, or verify</strong> these outputs.</p>
<p>Equally important, there was <strong>minimal insight into how tactile, auditory, and braille modalities</strong> independently supported users‚Äô cognitive and emotional engagement with data.</p>
<p>As a <strong>mixed-methods UX researcher</strong>, I positioned my work around filling these gaps systematically through <strong>multi-phase, user-centered investigations</strong>.</p>
</section>
<section id="research-objective" class="level3">
<h3 class="anchored" data-anchor-id="research-objective">Research Objective</h3>
<blockquote class="blockquote">
<p><strong>How can multimodal and AI-driven systems enable independent, trustworthy, and cognitively accessible data interpretation for BLV users?</strong></p>
</blockquote>
</section>
<section id="research-strategy-overview" class="level3">
<h3 class="anchored" data-anchor-id="research-strategy-overview">Research Strategy Overview</h3>
<p>Given the complexity, I structured the work into two <strong>sequential studies</strong> that informed each other:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Study</th>
<th style="text-align: left;">Key Focus</th>
<th style="text-align: left;">Methods Employed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Study 1</td>
<td style="text-align: left;">User interaction with AI-generated descriptions (maidrAI)</td>
<td style="text-align: left;">Semi-Structured Interviews, Thematic Analysis, AUS Survey</td>
</tr>
<tr class="even">
<td style="text-align: left;">Study 2</td>
<td style="text-align: left;">User performance across tactile, sonified, and braille data representations</td>
<td style="text-align: left;">Gesture Analysis, Demographic Survey, Statistical Analysis (Friedman Test)</td>
</tr>
</tbody>
</table>
<p><img src="img/maidr/maidr-hands.jpg" class="project-img img-fluid"></p>
</section>
<section id="study-1-evaluating-ai-generated-multimodal-descriptions-maidrai" class="level3">
<h3 class="anchored" data-anchor-id="study-1-evaluating-ai-generated-multimodal-descriptions-maidrai">Study 1: Evaluating AI-Generated Multimodal Descriptions (maidrAI)</h3>
<section id="problem-framing" class="level4">
<h4 class="anchored" data-anchor-id="problem-framing">Problem Framing</h4>
<p>Despite LLMs‚Äô potential to describe data, <strong>trust</strong> and <strong>personalization</strong> remained critical unknowns:</p>
<ul>
<li>Would users accept the AI‚Äôs interpretation at face value?<br>
</li>
<li>How might users want to prompt, edit, or verify descriptions?</li>
<li>Could verbose AI outputs increase cognitive fatigue?</li>
</ul>
<p>I hypothesized that <strong>customization and validation mechanisms</strong> would be key to successful adoption.</p>
</section>
<section id="methodological-approach" class="level4">
<h4 class="anchored" data-anchor-id="methodological-approach">Methodological Approach</h4>
<p><strong>Mixed Methods Design</strong>:<br>
I blended <strong>qualitative exploration</strong> with <strong>quantitative usability metrics</strong> to triangulate user experience:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Data Type</th>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Qualitative</td>
<td style="text-align: left;">Semi-Structured Interviews</td>
<td style="text-align: left;">Capture personalization needs, verification strategies</td>
</tr>
<tr class="even">
<td style="text-align: left;">Qualitative</td>
<td style="text-align: left;">Thematic Analysis</td>
<td style="text-align: left;">Derive design principles from user narratives</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Quantitative</td>
<td style="text-align: left;">Accessible Usability Scale (AUS)</td>
<td style="text-align: left;">Quantify satisfaction, usability, cognitive load</td>
</tr>
</tbody>
</table>
</section>
<section id="semi-structured-interviews" class="level4">
<h4 class="anchored" data-anchor-id="semi-structured-interviews">Semi-Structured Interviews</h4>
<p>After participants explored the maidrAI system, I conducted semi-structured interviews focusing on:</p>
<ul>
<li>How they adapted or prompted the AI</li>
<li>Strategies they used to verify output trustworthiness</li>
<li>Preferences around description styles and length</li>
</ul>
<p><strong>Emergent Themes</strong>:</p>
<ul>
<li><strong>Layered Summaries</strong>: Users preferred quick summaries with the option to drill down into detail.</li>
<li><strong>Verification Rituals</strong>: Users routinely ‚Äúdouble-checked‚Äù AI-generated information.</li>
<li><strong>Cognitive Load</strong>: Verbosity increased mental effort, especially when navigating complex datasets.</li>
</ul>
<p><img src="img/maidr/maidrai.jpg" class="project-img img-fluid"></p>
</section>
<section id="accessible-usability-scale-aus-survey" class="level4">
<h4 class="anchored" data-anchor-id="accessible-usability-scale-aus-survey">Accessible Usability Scale (AUS) Survey</h4>
<p>Quantitative triangulation confirmed qualitative insights:</p>
<ul>
<li><strong>Cognitive Overload</strong>: Participants rated maidrAI lower on cognitive simplicity.</li>
<li><strong>Task Completion</strong>: Higher scores for independence but lower scores for efficiency.</li>
</ul>
<p>This validated the urgent need for <strong>adaptive AI responses</strong> based on user cognitive bandwidth.</p>
</section>
<section id="key-insights-from-study-1" class="level4">
<h4 class="anchored" data-anchor-id="key-insights-from-study-1">Key Insights from Study 1</h4>
<p>‚úÖ <strong>AI outputs must be modular and customizable</strong> for BLV users to maintain autonomy.<br>
‚úÖ <strong>Trust-building in AI requires transparency and control, not just accuracy.</strong><br>
‚úÖ <strong>Cognitive load is a UX barrier</strong>, and reducing it should be a primary design goal.</p>
</section>
</section>
<section id="study-2-recognizing-statistical-properties-through-multimodal-interaction" class="level3">
<h3 class="anchored" data-anchor-id="study-2-recognizing-statistical-properties-through-multimodal-interaction">Study 2: Recognizing Statistical Properties Through Multimodal Interaction</h3>
<section id="problem-framing-1" class="level4">
<h4 class="anchored" data-anchor-id="problem-framing-1">Problem Framing</h4>
<p>Beyond textual descriptions, <strong>how well could BLV users interpret statistical properties</strong> (like skewness and modality) <strong>through tactile, sonified, or braille data representations</strong>?</p>
<p>I reasoned that performance differences across modalities could inform <strong>adaptive, user-selectable multimodal systems</strong>.</p>
</section>
<section id="methodological-approach-1" class="level4">
<h4 class="anchored" data-anchor-id="methodological-approach-1">Methodological Approach</h4>
<p><strong>Behavioural + Performance Data</strong><br>
I expanded from self-reported data to <strong>observable behaviours and performance outcomes</strong>.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Data Type</th>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Behavioural</td>
<td style="text-align: left;">Gesture Analysis</td>
<td style="text-align: left;">Map tactile and braille exploration strategies</td>
</tr>
<tr class="even">
<td style="text-align: left;">Behavioural</td>
<td style="text-align: left;">Playback Tracking</td>
<td style="text-align: left;">Analyze auditory navigation behaviour</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Performance</td>
<td style="text-align: left;">Accuracy, Confidence, Response Time</td>
<td style="text-align: left;">Evaluate task success across modalities</td>
</tr>
<tr class="even">
<td style="text-align: left;">Statistical</td>
<td style="text-align: left;">Friedman Test</td>
<td style="text-align: left;">Assess modality differences significance</td>
</tr>
</tbody>
</table>
</section>
<section id="gesture-analysis-and-playback-tracking" class="level4">
<h4 class="anchored" data-anchor-id="gesture-analysis-and-playback-tracking">Gesture Analysis and Playback Tracking</h4>
<ul>
<li>Designed 9 histograms representing different statistical shapes.</li>
<li>Created tactile and braille diagrams (swell-form paper) and sonified versions.</li>
<li>Recorded:
<ul>
<li><strong>Hand movement patterns</strong> (start points, looping, speed changes)</li>
<li><strong>Sonification playback repetitions</strong> and edits</li>
</ul></li>
</ul>
<p><strong>Behavioural Findings</strong>:</p>
<ul>
<li>Tactile diagrams promoted methodical, slow scanning patterns.</li>
<li>Sonification prompted rapid comparisons but sometimes missed fine details.</li>
<li>Braille readers relied heavily on numeric precision but struggled with ‚Äúbig-picture‚Äù trends.</li>
</ul>
<p><img src="img/maidr/histcognition.jpg" class="project-img img-fluid"></p>
</section>
<section id="quantitative-performance-analysis" class="level4">
<h4 class="anchored" data-anchor-id="quantitative-performance-analysis">Quantitative Performance Analysis</h4>
<p>Using the <strong>Friedman Test</strong>, I compared user performance:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Modality</th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;">Response Time</th>
<th style="text-align: left;">Confidence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Tactile</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">Slow</td>
<td style="text-align: left;">High</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sonification</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Moderate</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Braille</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Slow</td>
<td style="text-align: left;">Moderate</td>
</tr>
</tbody>
</table>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>No statistically significant difference in raw accuracy.</li>
<li>Significant modality-driven differences in <strong>confidence</strong> and <strong>speed</strong>.</li>
<li>Tactile representations improved comprehension but demanded higher cognitive effort.</li>
</ul>
</section>
</section>
<section id="integrating-findings-into-design" class="level3">
<h3 class="anchored" data-anchor-id="integrating-findings-into-design">Integrating Findings into Design</h3>
<p>Given user diversity, I concluded that <strong>a static one-size-fits-all modality would fail</strong>.<br>
Instead, I designed a <strong>dynamic dashboard prototype</strong> that lets users:</p>
<ul>
<li>Upload datasets</li>
<li>Choose between tactile, sonified, or text outputs</li>
<li>Customize verbosity, feedback, and navigation style</li>
</ul>
<p>üîó <a href="https://xabilitylab.shinyapps.io/a11y_dashboard/">Interactive Dashboard Prototype</a></p>
<p><img src="img/maidr/dashboard.jpg" class="project-img img-fluid"></p>
</section>
<section id="reflections-on-research-process" class="level3">
<h3 class="anchored" data-anchor-id="reflections-on-research-process">Reflections on Research Process</h3>
<section id="what-worked" class="level4">
<h4 class="anchored" data-anchor-id="what-worked">What Worked</h4>
<ul>
<li><strong>Sequential mixed methods</strong> let qualitative insights directly inform quantitative design.</li>
<li><strong>Behavioural observation + performance metrics</strong> revealed subtleties not captured in self-report.</li>
<li><strong>Collaborative design</strong> with BLV researchers and users grounded every decision in lived experience.</li>
</ul>
</section>
<section id="challenges" class="level4">
<h4 class="anchored" data-anchor-id="challenges">Challenges</h4>
<ul>
<li><strong>Managing cognitive load</strong> across different modalities required delicate balancing.</li>
<li><strong>Interpreting non-verbal behavioural data</strong> (gestures, replays) demanded careful cross-validation.</li>
</ul>
</section>
<section id="personal-learning" class="level4">
<h4 class="anchored" data-anchor-id="personal-learning">Personal Learning</h4>
<p>This project solidified my belief that <strong>trust, control, and personalization</strong> must drive accessible AI design ‚Äî especially when navigating cognitive and emotional complexity in assistive tech.</p>
</section>
</section>
<section id="final-outcome" class="level3">
<h3 class="anchored" data-anchor-id="final-outcome">Final Outcome</h3>
<p>‚úÖ Generated empirical insights on modality-specific strengths and weaknesses.<br>
‚úÖ Designed a user-driven dashboard enabling customizable multimodal data access.<br>
‚úÖ Contributed to the broader mission of equitable, ethical, and autonomous AI accessibility solutions.</p>
<p><a href="./projects.html">Back to Projects</a></p>
</section>
</div>
<hr>
</section>
<section id="designing-accessible-vr-exergames-for-blind-and-low-vision-blv-users" class="level2">
<h2 class="anchored" data-anchor-id="designing-accessible-vr-exergames-for-blind-and-low-vision-blv-users"><span id="voice-agent" title="Designing Accessible VR Exergames for Blind and Low Vision Users">Designing Accessible VR Exergames for Blind and Low Vision (BLV) Users</span></h2>
<p><img src="img/vr/boxing.jpg" class="project-img img-fluid"></p>
<div class="project-details">
<p><span class="article-tag">Research Ethics</span> <span class="article-tag">Virtual Reality</span> <span class="article-tag">Haptic Design</span> <span class="article-tag">Spatial Audio</span> <span class="article-tag">Physical Engagement</span></p>
<p><em>March 2023 - January 2024</em></p>
<section id="project-context-1" class="level3">
<h3 class="anchored" data-anchor-id="project-context-1">Project Context</h3>
<p>Virtual Reality (VR) offers incredible potential for immersive physical engagement.<br>
Yet, for Blind and Low Vision (BLV) users, VR often remains an <strong>exclusionary medium</strong>, heavily reliant on visual cues.</p>
<p>When I joined this project, I recognized a powerful opportunity:<br>
<strong>Could we create skill-based, independently playable VR exergames for BLV users, using multimodal (audio + haptic) interaction?</strong></p>
</section>
<section id="research-objective-1" class="level3">
<h3 class="anchored" data-anchor-id="research-objective-1">Research Objective</h3>
<blockquote class="blockquote">
<p><strong>How can multimodal sensory feedback enable accessible, skill-driven VR sports gameplay for BLV users, without compromising realism or agency?</strong></p>
</blockquote>
</section>
<section id="research-strategy-overview-1" class="level3">
<h3 class="anchored" data-anchor-id="research-strategy-overview-1">Research Strategy Overview</h3>
<p>Given the complexity of real-time movement, feedback, and immersion in VR, I structured research into two major phases:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Study</th>
<th style="text-align: left;">Key Focus</th>
<th style="text-align: left;">Methods</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Study 1</td>
<td style="text-align: left;">Accessible VR Table Tennis Simulation</td>
<td style="text-align: left;">Participatory Co-Design, Iterative Usability Testing, Spatial Audio + Haptic Design</td>
</tr>
<tr class="even">
<td style="text-align: left;">Study 2</td>
<td style="text-align: left;">Accessible VR Boxing for Physical Engagement</td>
<td style="text-align: left;">Participatory Co-Design, Game Analytics, Physical Activity Measurement, Qualitative Immersion Interviews</td>
</tr>
</tbody>
</table>
</section>
<section id="study-1-accessible-vr-table-tennis-game" class="level3">
<h3 class="anchored" data-anchor-id="study-1-accessible-vr-table-tennis-game">Study 1: Accessible VR Table Tennis Game</h3>
<section id="problem-framing-2" class="level4">
<h4 class="anchored" data-anchor-id="problem-framing-2">Problem Framing</h4>
<p>Current VR adaptations for accessibility often introduce ‚Äúpatches‚Äù after development.<br>
I hypothesized: <strong>True BLV accessibility demands integrated design from the outset ‚Äî making audio and haptics foundational, not secondary.</strong></p>
</section>
<section id="methodological-approach-2" class="level4">
<h4 class="anchored" data-anchor-id="methodological-approach-2">Methodological Approach</h4>
<p><strong>Mixed Methods Participatory Research</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Data Type</th>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Qualitative</td>
<td style="text-align: left;">Co-Design Sessions with BLV Players</td>
<td style="text-align: left;">Shape mechanics, feedback loops</td>
</tr>
<tr class="even">
<td style="text-align: left;">Behavioural</td>
<td style="text-align: left;">Usability Observations</td>
<td style="text-align: left;">Identify interaction barriers</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sensory</td>
<td style="text-align: left;">Sensory Feedback Calibration</td>
<td style="text-align: left;">Optimize spatial audio + haptic timing</td>
</tr>
<tr class="even">
<td style="text-align: left;">Iterative</td>
<td style="text-align: left;">Rapid Prototyping + Testing</td>
<td style="text-align: left;">Agile UX improvements</td>
</tr>
</tbody>
</table>
</section>
<section id="participatory-co-design" class="level4">
<h4 class="anchored" data-anchor-id="participatory-co-design">Participatory Co-Design</h4>
<p>I conducted <strong>weekly participatory sessions</strong> with BLV co-designers over four months:</p>
<ul>
<li>Users tested early builds and prototypes.</li>
<li>Feedback loops focused on sensory clarity, reaction time, and situational awareness.</li>
</ul>
<p><strong>Emergent Priorities</strong>:</p>
<ul>
<li><strong>Spatial audio must clearly track ball movement</strong> ‚Äî pitch, distance, velocity.</li>
<li><strong>Haptic feedback must guide paddle contact strength and timing.</strong></li>
<li><strong>Reduce visual noise</strong> to minimize cognitive clutter for low-vision players.</li>
</ul>
</section>
<section id="sensory-integration-design" class="level4">
<h4 class="anchored" data-anchor-id="sensory-integration-design">Sensory Integration Design</h4>
<p><img src="img/vr/feedback.png" class="project-img img-fluid"></p>
<section id="spatial-audio-unity-engine" class="level5">
<h5 class="anchored" data-anchor-id="spatial-audio-unity-engine">1. Spatial Audio (Unity Engine)</h5>
<ul>
<li>Ball tracking was tied to <strong>real-time 3D audio positioning</strong>.</li>
<li>Doppler effect simulation conveyed <strong>ball speed</strong> and <strong>directional drift</strong>.</li>
<li>Paddle had <strong>locational audio buzz</strong> to help players orient.</li>
</ul>
</section>
<section id="haptic-feedback-bhaptics-studio" class="level5">
<h5 class="anchored" data-anchor-id="haptic-feedback-bhaptics-studio">2. Haptic Feedback (bHaptics Studio)</h5>
<ul>
<li>Paddle-ball contact strength modulated vibration intensity.</li>
<li>Incoming ball proximity triggered <strong>progressive pulses</strong>.</li>
<li>Missed shots activated <strong>distinct low-frequency vibration</strong> for corrective feedback.</li>
</ul>
</section>
</section>
<section id="usability-testing-and-iteration" class="level4">
<h4 class="anchored" data-anchor-id="usability-testing-and-iteration">Usability Testing and Iteration</h4>
<p>During testing: - Players initially <strong>overreacted</strong> to Doppler shifts. - Some participants <strong>misjudged ball bounce height</strong> due to uniform audio cues.</p>
<p><strong>Design Refinements</strong>: - Added <strong>low-pass filters</strong> for bounce vs.&nbsp;paddle impact sound. - Introduced <strong>boundary haptics</strong> to prevent table disorientation.</p>
</section>
<section id="outcomes-from-study-1" class="level4">
<h4 class="anchored" data-anchor-id="outcomes-from-study-1">Outcomes from Study 1</h4>
<p>‚úÖ Created a <strong>realistic, skill-based VR Table Tennis experience</strong> for BLV players without visual reliance.<br>
‚úÖ Established foundational <strong>multimodal sensory design principles</strong> for accessible VR sports.</p>
</section>
</section>
<section id="study-2-accessible-vr-boxing-game" class="level3">
<h3 class="anchored" data-anchor-id="study-2-accessible-vr-boxing-game">Study 2: Accessible VR Boxing Game</h3>
<section id="problem-framing-3" class="level4">
<h4 class="anchored" data-anchor-id="problem-framing-3">Problem Framing</h4>
<p>Table Tennis enabled technical reaction skills ‚Äî but lacked sustained physical exertion.<br>
I hypothesized: <strong>VR Boxing could offer a more physically engaging experience, if designed with accessible non-visual navigation.</strong></p>
</section>
<section id="methodological-approach-3" class="level4">
<h4 class="anchored" data-anchor-id="methodological-approach-3">Methodological Approach</h4>
<p><strong>Full Mixed Methods Study</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Data Type</th>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Behavioural</td>
<td style="text-align: left;">Co-Design Iterations</td>
<td style="text-align: left;">Optimize opponent tracking, haptic hit feedback</td>
</tr>
<tr class="even">
<td style="text-align: left;">Quantitative</td>
<td style="text-align: left;">Physical Activity (PA) Tracking</td>
<td style="text-align: left;">Measure exertion levels (Heart Rate, Energy Use)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Quantitative</td>
<td style="text-align: left;">Immersion Assessment (ITC-SOPI Survey)</td>
<td style="text-align: left;">Evaluate presence and emotional engagement</td>
</tr>
<tr class="even">
<td style="text-align: left;">Qualitative</td>
<td style="text-align: left;">Immersion Interviews</td>
<td style="text-align: left;">Surface emotional responses, strategy variations</td>
</tr>
</tbody>
</table>
</section>
<section id="co-design-and-system-implementation" class="level4">
<h4 class="anchored" data-anchor-id="co-design-and-system-implementation">Co-Design and System Implementation</h4>
<ul>
<li>Designed a <strong>clock-based spatial audio cueing system</strong> (e.g., ‚Äúattack incoming at 3 o‚Äôclock‚Äù).</li>
<li>Integrated <strong>adaptive difficulty settings</strong> (reaction speeds, cue intensity) based on user skill.</li>
<li>Developed <strong>real-time haptic response</strong> tied to punch impact and dodges.</li>
</ul>
<p><img src="img/vr/headset.png" class="project-img img-fluid"></p>
</section>
<section id="evaluation-physical-activity-and-immersion" class="level4">
<h4 class="anchored" data-anchor-id="evaluation-physical-activity-and-immersion">Evaluation: Physical Activity and Immersion</h4>
<section id="physical-effort-metrics" class="level5">
<h5 class="anchored" data-anchor-id="physical-effort-metrics">1. Physical Effort Metrics</h5>
<ul>
<li>Monitored Heart Rate across sessions.</li>
<li>Compared physical activity expenditure between two participants (different gameplay strategies).</li>
</ul>
<p><img src="img/vr/physicaleffort.jpg" class="project-img img-fluid"></p>
</section>
<section id="user-immersion-itc-sopi-survey" class="level5">
<h5 class="anchored" data-anchor-id="user-immersion-itc-sopi-survey">2. User Immersion (ITC-SOPI Survey)</h5>
<ul>
<li>Participants scored high on:
<ul>
<li>Sense of physical presence</li>
<li>Perceived realism</li>
<li>Engagement and emotional immersion</li>
</ul></li>
</ul>
</section>
<section id="qualitative-feedback" class="level5">
<h5 class="anchored" data-anchor-id="qualitative-feedback">3. Qualitative Feedback</h5>
<ul>
<li>Players felt <strong>empowered and independent</strong>.</li>
<li>‚ÄúThe audio made it feel like I was in a real fight,‚Äù remarked one participant.</li>
</ul>
</section>
</section>
<section id="outcomes-from-study-2" class="level4">
<h4 class="anchored" data-anchor-id="outcomes-from-study-2">Outcomes from Study 2</h4>
<p>‚úÖ Validated that <strong>spatial audio + adaptive haptics can enable physically active, immersive VR fitness</strong> for BLV users.<br>
‚úÖ Demonstrated <strong>different play styles (aggressive vs.&nbsp;strategic)</strong> across participants, reinforcing the need for flexible difficulty tuning.</p>
</section>
</section>
<section id="reflections-on-research-process-1" class="level3">
<h3 class="anchored" data-anchor-id="reflections-on-research-process-1">Reflections on Research Process</h3>
<section id="what-worked-1" class="level4">
<h4 class="anchored" data-anchor-id="what-worked-1">What Worked</h4>
<ul>
<li><strong>Multimodal layering</strong> created intuitive situational awareness.</li>
<li><strong>Co-design empowered real user voices</strong> throughout mechanical iterations.</li>
</ul>
</section>
<section id="challenges-1" class="level4">
<h4 class="anchored" data-anchor-id="challenges-1">Challenges</h4>
<ul>
<li><strong>Clock-cue training curves</strong> varied by individual; learning time required.</li>
<li><strong>Managing physical safety</strong> during dynamic movement in a sightless environment demanded extra safeguards.</li>
</ul>
</section>
<section id="personal-learning-1" class="level4">
<h4 class="anchored" data-anchor-id="personal-learning-1">Personal Learning</h4>
<p>True VR accessibility is <strong>not about replacing vision</strong>, but about <strong>expanding sensory frameworks intelligently</strong> ‚Äî trusting users‚Äô spatial intelligence beyond sight.</p>
<p><a href="./projects.html">Back to Projects</a></p>
</section>
</section>
</div>
<hr>
</section>
<section id="clearminds-designing-trustworthy-digital-mental-health-support" class="level2">
<h2 class="anchored" data-anchor-id="clearminds-designing-trustworthy-digital-mental-health-support"><span id="autistic-education" title="ClearMinds: Designing Trustworthy Digital Mental Health Support">ClearMinds: Designing Trustworthy Digital Mental Health Support</span></h2>
<div class="project-details">
<p><span class="article-tag">Research Ethics</span> <span class="article-tag">Mental Health UX</span> <span class="article-tag">Empathic Design</span> <span class="article-tag">Emotional Interfacing </span> <span class="article-tag">User Research</span></p>
<p><em>March 2023 - June 2023</em></p>
<section id="project-context-2" class="level3">
<h3 class="anchored" data-anchor-id="project-context-2">Project Context</h3>
<p>Mental health apps often face two core issues:<br>
- <strong>Users struggle to trust</strong> generalized, AI-driven recommendations.<br>
- <strong>Tracking emotional progress</strong> feels either too mechanical or too overwhelming.</p>
<p>When I conceptualized ClearMinds, I asked:<br>
<strong>Can we create a system that feels structured yet human, scientific yet emotionally validating?</strong></p>
</section>
<section id="research-objective-2" class="level3">
<h3 class="anchored" data-anchor-id="research-objective-2">Research Objective</h3>
<blockquote class="blockquote">
<p><strong>How can mental wellness platforms balance structure, trust, and emotional nuance to support sustained, meaningful user engagement?</strong></p>
</blockquote>
</section>
<section id="research-strategy-overview-2" class="level3">
<h3 class="anchored" data-anchor-id="research-strategy-overview-2">Research Strategy Overview</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Phase</th>
<th style="text-align: left;">Focus</th>
<th style="text-align: left;">Methods</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1</td>
<td style="text-align: left;">Understand User Pain Points</td>
<td style="text-align: left;">Persona Building, Journey Mapping</td>
</tr>
<tr class="even">
<td style="text-align: left;">2</td>
<td style="text-align: left;">Ideate Structure</td>
<td style="text-align: left;">Affinity Diagramming, Storyboarding</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3</td>
<td style="text-align: left;">Prototype and Test</td>
<td style="text-align: left;">Paper Wireframing, Low-Fidelity UX Testing</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">Refine and Evaluate</td>
<td style="text-align: left;">High-Fidelity Prototyping, Accessibility Audits, Usability Testing</td>
</tr>
</tbody>
</table>
<p><img src="img/clearminds/userflow.png" class="project-img img-fluid"></p>
</section>
<section id="phase-1-discovery-understanding-emotional-and-structural-needs" class="level3">
<h3 class="anchored" data-anchor-id="phase-1-discovery-understanding-emotional-and-structural-needs">Phase 1: Discovery ‚Äî Understanding Emotional and Structural Needs</h3>
<section id="persona-creation" class="level4">
<h4 class="anchored" data-anchor-id="persona-creation">Persona Creation</h4>
<p>Through early interviews and secondary research, I created core user personas:</p>
<ul>
<li><strong>Trust-Seeker</strong>: Skeptical of digital therapy.</li>
<li><strong>Structure-Seeker</strong>: Wants clear progress pathways.</li>
<li><strong>Flexibility-Seeker</strong>: Needs adaptable emotional tracking tools.</li>
</ul>
<p><img src="img/clearminds/persona.png" class="project-img img-fluid"></p>
</section>
<section id="journey-mapping" class="level4">
<h4 class="anchored" data-anchor-id="journey-mapping">Journey Mapping</h4>
<p>Mapped emotional highs and lows across user experiences with existing apps.<br>
Critical ‚Äúdrop-off‚Äù points: - Feeling misunderstood by generic AI feedback. - Overwhelmed by rigid task flows without customization.</p>
</section>
</section>
<section id="phase-2-ideation-structuring-flexible-support" class="level3">
<h3 class="anchored" data-anchor-id="phase-2-ideation-structuring-flexible-support">Phase 2: Ideation ‚Äî Structuring Flexible Support</h3>
<section id="affinity-diagramming" class="level4">
<h4 class="anchored" data-anchor-id="affinity-diagramming">Affinity Diagramming</h4>
<p>Grouped user needs into thematic clusters:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Theme</th>
<th style="text-align: left;">Implication</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Trust &amp; Transparency</td>
<td style="text-align: left;">Show users how recommendations are formed</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Flexible Structure</td>
<td style="text-align: left;">Allow self-paced, customizable progress tracking</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Empowerment</td>
<td style="text-align: left;">Offer users second-opinion mechanisms and therapist switch options</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</section>
<section id="storyboarding" class="level4">
<h4 class="anchored" data-anchor-id="storyboarding">Storyboarding</h4>
<p>Visualized use cases where structured guidance adapts dynamically to emotional states.</p>
<p><img src="img/clearminds/storyboard.png" class="project-img img-fluid"></p>
</section>
</section>
<section id="phase-3-prototyping-testing-core-concepts" class="level3">
<h3 class="anchored" data-anchor-id="phase-3-prototyping-testing-core-concepts">Phase 3: Prototyping ‚Äî Testing Core Concepts</h3>
<section id="paper-wireframes-and-low-fidelity-prototypes" class="level4">
<h4 class="anchored" data-anchor-id="paper-wireframes-and-low-fidelity-prototypes">Paper Wireframes and Low-Fidelity Prototypes</h4>
<ul>
<li>Created progress dashboards</li>
<li>Designed structured task boards with opt-out flexibility</li>
<li>Developed ‚Äútherapist transparency‚Äù profiles for clearer communication</li>
</ul>
</section>
<section id="usability-testing" class="level4">
<h4 class="anchored" data-anchor-id="usability-testing">Usability Testing</h4>
<p>Tested wireframes with real users.<br>
Major learnings: - Needed visual metaphors for emotional states (e.g., weather systems). - Mandatory task tracking discouraged engagement; flexibility was crucial.</p>
</section>
</section>
<section id="phase-4-refinement-building-trust-and-accessibility" class="level3">
<h3 class="anchored" data-anchor-id="phase-4-refinement-building-trust-and-accessibility">Phase 4: Refinement ‚Äî Building Trust and Accessibility</h3>
<section id="high-fidelity-prototyping" class="level4">
<h4 class="anchored" data-anchor-id="high-fidelity-prototyping">High-Fidelity Prototyping</h4>
<ul>
<li>Integrated second-opinion features for therapy paths.</li>
<li>Gamified emotional tracking with visual progress markers.</li>
<li>Added optionality across every structure (skip, edit, pause).</li>
</ul>
</section>
<section id="accessibility-audits" class="level4">
<h4 class="anchored" data-anchor-id="accessibility-audits">Accessibility Audits</h4>
<ul>
<li>Ensured WCAG 2.1 AA compliance.</li>
<li>Designed dark mode and dyslexia-friendly font options.</li>
</ul>
<p><img src="img/clearminds/hifi.jpg" class="project-img img-fluid"></p>
</section>
</section>
<section id="reflections-on-research-process-2" class="level3">
<h3 class="anchored" data-anchor-id="reflections-on-research-process-2">Reflections on Research Process</h3>
<section id="what-worked-2" class="level4">
<h4 class="anchored" data-anchor-id="what-worked-2">What Worked</h4>
<ul>
<li><strong>Building transparency and control</strong> built trust faster than ‚Äúsmarter‚Äù AI claims.</li>
<li><strong>Flexible structure</strong> encouraged emotional authenticity and long-term engagement.</li>
</ul>
</section>
<section id="challenges-2" class="level4">
<h4 class="anchored" data-anchor-id="challenges-2">Challenges</h4>
<ul>
<li><strong>Balancing emotional depth with UX simplicity</strong> required careful emotional design without over-engineering.</li>
<li><strong>Habit formation</strong> needed gentle nudging, not rigid enforcement.</li>
</ul>
</section>
<section id="personal-learning-2" class="level4">
<h4 class="anchored" data-anchor-id="personal-learning-2">Personal Learning</h4>
<p>Mental health UX must respect <strong>emotional messiness</strong> ‚Äî users thrive when systems flex to accommodate, not correct, their inner journeys.</p>
<p><a href="./projects.html">Back to Projects</a></p>
</section>
</section>
</div>
<hr>
<div class="view-all-projects">
<p><a href="./projects.html" class="hover-lift">View All Projects</a></p>
</div>


</section>
</div>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/SK-143381\.github\.io\/sk-143381\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<script src="animations.js"></script>
<script src="hover-enhancements.js"></script>




</body></html>