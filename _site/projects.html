<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.427">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Sanchita Kamath | UX Researcher - Research Portfolio üéì</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./img/favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;500;600;700&amp;family=Roboto:wght@300;400;500;700&amp;family=Open+Sans:wght@400;500;600;700&amp;display=swap" rel="stylesheet">


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./img/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Sanchita Kamath | UX Researcher</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./projects.html" rel="" target="" aria-current="page">
 <span class="menu-text">Work üöÄ</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./research.html" rel="" target="">
 <span class="menu-text">Research üîç</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./writing.html" rel="" target="">
 <span class="menu-text">Writings üìì</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./cv.html" rel="" target="">
 <span class="menu-text">Resume</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/sanchitakamath/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:mail.kamath.sanchita@gmail.com" rel="" target=""><i class="bi bi-envelope" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/yourgithubusername" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#research-overview" id="toc-research-overview" class="nav-link active" data-scroll-target="#research-overview">Research Overview</a></li>
  <li><a href="#tripbot-conversational-interface-for-blv-travel-assistance" id="toc-tripbot-conversational-interface-for-blv-travel-assistance" class="nav-link" data-scroll-target="#tripbot-conversational-interface-for-blv-travel-assistance">TripBot: Conversational Interface for BLV Travel Assistance</a></li>
  <li><a href="#rlhf-voice-agent-emotional-intelligence-in-human-computer-interaction" id="toc-rlhf-voice-agent-emotional-intelligence-in-human-computer-interaction" class="nav-link" data-scroll-target="#rlhf-voice-agent-emotional-intelligence-in-human-computer-interaction">RLHF Voice Agent: Emotional Intelligence in Human-Computer Interaction</a></li>
  <li><a href="#learning-through-technology-to-enhance-skill-development-in-autistic-females" id="toc-learning-through-technology-to-enhance-skill-development-in-autistic-females" class="nav-link" data-scroll-target="#learning-through-technology-to-enhance-skill-development-in-autistic-females">Learning Through Technology to Enhance Skill Development in Autistic Females</a></li>
  <li><a href="#smart-shoe-biomechanical-analysis-in-preventive-healthcare" id="toc-smart-shoe-biomechanical-analysis-in-preventive-healthcare" class="nav-link" data-scroll-target="#smart-shoe-biomechanical-analysis-in-preventive-healthcare">Smart Shoe: Biomechanical Analysis in Preventive Healthcare</a></li>
  <li><a href="#iot-based-door-locking-using-computer-vision" id="toc-iot-based-door-locking-using-computer-vision" class="nav-link" data-scroll-target="#iot-based-door-locking-using-computer-vision">IoT-Based Door Locking using Computer Vision</a></li>
  <li><a href="#m.-hostels-dynamic-pricing-in-hospitality-management" id="toc-m.-hostels-dynamic-pricing-in-hospitality-management" class="nav-link" data-scroll-target="#m.-hostels-dynamic-pricing-in-hospitality-management">M. HOSTELS: Dynamic Pricing in Hospitality Management</a></li>
  <li><a href="#optima-universal-design-in-time-management" id="toc-optima-universal-design-in-time-management" class="nav-link" data-scroll-target="#optima-universal-design-in-time-management">OPTIMA: Universal Design in Time Management</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Research Portfolio üéì</h1>
<p class="subtitle lead">UX Research &amp; Development Projects</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="research-overview" class="level2">
<h2 class="anchored" data-anchor-id="research-overview">Research Overview</h2>
<p>My research portfolio represents a comprehensive exploration of human-computer interaction, accessibility, and cognitive science applications in technology development. Each project is grounded in theoretical frameworks and empirical evidence, with a focus on measurable outcomes and user-centered design principles.</p>
<hr>
</section>
<div id="project1" class="project-container">
<section id="tripbot-conversational-interface-for-blv-travel-assistance" class="level2">
<h2 class="anchored" data-anchor-id="tripbot-conversational-interface-for-blv-travel-assistance">TripBot: Conversational Interface for BLV Travel Assistance</h2>
<div class="project-details">
<p><span class="project-tag">Human-Computer Interaction</span> <span class="project-tag">Conversational AI</span> <span class="project-tag">Large Language Models</span> <span class="project-tag">User Research</span> <span class="project-tag">Assistive Technology</span></p>
<p><em>January 2024 - Present</em></p>
<section id="research-context" class="level3">
<h3 class="anchored" data-anchor-id="research-context">Research Context</h3>
<p>The development of TripBot began with identifying a critical gap in existing travel assistance solutions for Blind and Low Vision (BLV) individuals. While current solutions provided basic navigation assistance, they often failed to address the need for real-time environmental awareness and contextual information. This limitation became particularly apparent during initial auto-ethnography conducted with a BLV team member, who expressed frustration with systems that couldn‚Äôt adapt to dynamic urban environments or provide meaningful context about their surroundings.</p>
</section>
<section id="research-process-design-decisions" class="level3">
<h3 class="anchored" data-anchor-id="research-process-design-decisions">Research Process &amp; Design Decisions</h3>
<p>Through our auto-ethnography, I derived key insights:</p>
<ol type="1">
<li><p><strong>Environmental Context Needs</strong></p>
<ul>
<li>BLV individuals want to understand not just their stand-alone location, but the surrounding context.</li>
<li>There is no standardized resource for BLV people to inquire about cane tips and contextualized and accurate travel guidance based on exact times of travel.</li>
<li>BLV individuals wish to gain information about potential obstacles or changes in the environment based on their plans and want to be prepared for their trip in advance.</li>
</ul></li>
<li><p><strong>Technical Approach Selection</strong> Based on these findings, we chose to develop a conversational interface for several reasons:</p>
<ul>
<li>Natural language allows for more nuanced communication of environmental context.</li>
<li>Conversational interfaces can adapt to different levels of detail based on user needs and is the primary mode of learning for BLV people.</li>
<li>The format enables two-way communication, allowing users to ask for specific information and providing a channel for relaying accurate feedback.</li>
</ul></li>
<li><p><strong>LLM Integration Decision</strong> The Initial testing with GPT-3.5 showed limitations in understanding spatial relationships and answers riddled with hallucinations. I therefore prompt engineered and developed a Retrieval-Augmented Generation piple for leveraging LLaMA-2‚Äôs contenxtual capabilities allowed for better processing of environmental context. I was able to generate reduced hallucinations through inputting weather data directly via the OpenWeatherMap API and Orientation and Mobility (O &amp; M) assistive technology dataset (which was curated by a team member). However, we still faced certain devolving conversations where the LLM could not remember context.</p></li>
<li><p><strong>Findings</strong> The internal system testing indicated varying preferences for detail levels where team members wished to customize the verbosity of the LLM. Another theme of the need to balance information density with cognitive load emerged, which highlighted the importance of personalizing the experience for different users.</p></li>
</ol>
</section>
<section id="implementation-challenges-solutions" class="level3">
<h3 class="anchored" data-anchor-id="implementation-challenges-solutions">Implementation Challenges &amp; Solutions</h3>
<p>During the implementation phase, we encountered several challenges:</p>
<ol type="1">
<li><strong>Environmental Understanding</strong>
<ul>
<li>Challenge: Accurately interpreting information about complex urban environments the user wants to traverse.</li>
<li>Solution: Developed an approach of feeding data for RAG in which accurate data was dynamically fetched and fed into the LLM.</li>
<li>Result: Improved accuracy in identifying and describing environmental features.</li>
</ul></li>
<li><strong>Response Generation</strong>
<ul>
<li>Challenge: Creating natural, informative responses without overwhelming users.</li>
<li>Solution: Implemented a context-aware response system that considers:
<ul>
<li>User‚Äôs current activity</li>
<li>Environmental complexity</li>
<li>User‚Äôs stated preferences</li>
</ul></li>
<li>Result: More natural and useful responses that adapt to the situation that allows users to initiate and direct conversation.</li>
</ul></li>
<li><strong>User Testing &amp; Iteration</strong> The system underwent three major iterations based on user feedback:
<ul>
<li>First iteration: Focused on basic navigation and environmental description.</li>
<li>Second iteration: Added preference-based customization.</li>
<li>Third iteration: Implemented continuous learning from user interactions (text-based).</li>
</ul></li>
</ol>
</section>
<section id="current-status-future-directions" class="level3">
<h3 class="anchored" data-anchor-id="current-status-future-directions">Current Status &amp; Future Directions</h3>
<p>The project is currently in the pilot testing phase, with several key areas of focus:</p>
<ol type="1">
<li><strong>System Performance</strong>
<ul>
<li>Evaluating response accuracy in various urban environments.</li>
<li>Implementing a fine-tuned LLaVA model to provide a platform for multimodal streaming.</li>
<li>Measuring user satisfaction with different types of information by analyzing user responses through vanilla sentiment analysis and NLU algorithms for tone analysis and topic modelling.</li>
<li>Assessing the effectiveness of the system through transformer-based benchmarking.</li>
</ul></li>
<li><strong>User Experience</strong>
<ul>
<li>Gathering feedback on the conversational interface.</li>
<li>Identifying areas for improvement in response generation.</li>
<li>Understanding how users adapt to the system over time. I wish to understand if system familiarity should be considered as an independent variable.</li>
</ul></li>
</ol>
</section>
</div>
</section>
</div>
<div id="project2" class="project-container">
<section id="rlhf-voice-agent-emotional-intelligence-in-human-computer-interaction" class="level2">
<h2 class="anchored" data-anchor-id="rlhf-voice-agent-emotional-intelligence-in-human-computer-interaction">RLHF Voice Agent: Emotional Intelligence in Human-Computer Interaction</h2>
<p><img src="img/rlhf/2_VCA_framework.png" class="project-img img-fluid" alt="Flowchart diagram of the smart mirror's conversational agent framework. The process starts when the user utters a 'trigger' word, activating the mirror's functionality. The system initiates a dialogue, leading to a voice-enabled conversation. An emotional label is attached to the user's input. The system then judges the conversational requirement: if empathy is needed, it continues the conversation; if redirection is needed, it stores attention values and either redirects or ends the conversation. The flow includes decision points for empathizing, redirecting, or ending the conversation, illustrating how the agent uses emotion and attention data to guide interaction."></p>
<div class="project-details">
<p><span class="project-tag">Human-Computer Interaction</span> <span class="project-tag">Reinforcement Learning</span> <span class="project-tag">Emotion Recognition</span> <span class="project-tag">Voice Interfaces</span> <span class="project-tag">UX Design</span></p>
<p><em>March 2023 - January 2024</em></p>
<section id="research-context-1" class="level3">
<h3 class="anchored" data-anchor-id="research-context-1">Research Context</h3>
<p>I‚Äôve always been fascinated by how emotionally intelligent systems could transform everyday interactions. What really pushed me to work on this smart mirror was the gap I noticed between sentiment-aware systems and systems that act empathetically in real time. Most systems stop at recognizing emotion ‚Äî they don‚Äôt respond meaningfully to it, and almost none link that to cognitive load or user attention. I felt there was untapped potential here, especially in a device as unobtrusive and personal as a mirror.</p>
</section>
<section id="research-process-design-decisions-1" class="level3">
<h3 class="anchored" data-anchor-id="research-process-design-decisions-1">Research Process &amp; Design Decisions</h3>
<p>I started by asking: ‚ÄúHow can I design something that feels natural and helpful, not forced or robotic?‚Äù This led me to center the user‚Äôs emotion and attention as core signals, integrating multiple modalities ‚Äî speech, facial expression, gaze ‚Äî to get a richer sense of the user‚Äôs state. The project began with a four-month development phase, followed by a single testing session with 5 participants (ages 20-25).</p>
<ol type="1">
<li><p><strong>Emotional Recognition Needs</strong></p>
<ul>
<li>Users wanted more than just voice commands; they sought emotional understanding</li>
<li>There was a need for real-time emotional state recognition</li>
<li>Participants expressed desire for supportive responses that maintained conversation context</li>
<li>Testing revealed that users with neutral or happy moods showed better recall, suggesting cognitive benefits</li>
</ul></li>
<li><p><strong>Emotion Recognition Performance</strong> For speech emotion recognition, I chose four robust open-source datasets. My reasoning:</p>
<ul>
<li>Real human voices across emotions provide enough variance</li>
<li>These datasets offered high-quality, labeled emotional recordings</li>
<li>I trained the model using Keras Sequential, prioritizing interpretability and rapid iteration</li>
</ul>
<p><img src="img/rlhf/3_confusion_matrix.png" class="confusion-matrix img-fluid" alt="Confusion matrix for emotion recognition. Rows represent actual emotions (Anger, Fear, Happiness, Neutral, Sadness), columns represent predicted emotions. The matrix shows high accuracy for Anger (138 correct), Fear (105), Happiness (98), Neutral (86), and Sadness (111). Some misclassifications occur, such as Happiness being confused with Fear (50 times) and Sadness with Fear (34 times). The color intensity represents the number of samples, with darker blue indicating higher counts. This visualization helps assess which emotions are most and least accurately classified by the model."></p>
<p>Key observations:</p>
<ul>
<li>Highest accuracy in recognizing neutral (85%) and happy (85%) states</li>
<li>Some confusion between sad and angry states (10% misclassification)</li>
<li>Overall accuracy of 81.25% across all emotional states</li>
</ul></li>
<li><p><strong>Technical Approach Selection</strong> I chose to develop a smart mirror interface because people look in the mirror every day. It‚Äôs a private, habitual, emotionally raw moment. That‚Äôs exactly where emotional technology should be ‚Äî not in a sterile app, but integrated into daily life. A mirror also gave me:</p>
<ul>
<li>A large, passive screen for gaze tracking</li>
<li>A way to observe subtle emotional shifts</li>
<li>A familiar object ‚Äî people wouldn‚Äôt be intimidated by the tech</li>
</ul>
<p><img src="img/rlhf/3_screen.png" class="interface-img img-fluid" alt="Screenshot of the smart mirror interface during a user session. The top left shows a webcam feed with a green square around the user's face and facial landmarks detected, indicating real-time facial emotion analysis. Below, a math problem (1 - 6 = ?) is displayed with two answer options, -4 and -5, each in a box. A red dot appears near the -5 option, representing the user's gaze or selection. This interface demonstrates the integration of facial analysis and attention tracking during a cognitive task."></p>
<p>I intentionally avoided overly complex UI, keeping visual clutter minimal. The goal was to let the conversation be the main interface.</p></li>
<li><p><strong>Attention Monitoring System</strong> I knew that emotion alone doesn‚Äôt tell the whole story. So I thought: Can I also track how engaged the user is? That‚Äôs where eye-tracking came in. I developed attention metrics based on:</p>
<ul>
<li>Focus (how long someone stays on a task)</li>
<li>Task-switching (how easily they shift between dialogue and problem-solving)</li>
<li>Processing speed</li>
<li>A composite metric (AM) with weights I could adjust later</li>
</ul>
<p><img src="img/rlhf/3_time_seg.jpg" class="interface-img img-fluid" alt="Bar chart titled 'Time Segregation for each user based on emotion.' The x-axis lists five participants (P1 to P5). Each bar is divided into three colored segments: blue for Focus Time (ms), cyan for Process Time (ms), and green for Task Switching Time (ms). The chart shows that Process Time is the largest component for most users, while Task Switching Time varies, being highest for P4. This visualization quantifies and compares how users allocate their time across different cognitive activities during the experiment."></p></li>
<li><p><strong>RLHF Implementation Decision</strong> I opted for a Partial Observable Markov Decision Process (POMDP) because I was dealing with uncertainty ‚Äî I wouldn‚Äôt always know the user‚Äôs true state, but I‚Äôd have probabilities. That‚Äôs real life, right? The action set was deliberately simple: empathize or redirect. My thought was: instead of trying to fake deep empathy with shallow scripts, the system should:</p>
<ul>
<li>Continue the topic gently if the user seems open</li>
<li>Change the subject or backtrack if things felt tense or stagnant</li>
<li>This made the CA feel more natural and respectful</li>
</ul></li>
</ol>
</section>
<section id="implementation-challenges-solutions-1" class="level3">
<h3 class="anchored" data-anchor-id="implementation-challenges-solutions-1">Implementation Challenges &amp; Solutions</h3>
<p>During the implementation phase, I encountered several challenges:</p>
<ol type="1">
<li><strong>Speech Analysis</strong>
<ul>
<li>Challenge: Real-time emotion recognition from speech</li>
<li>Solution: Developed an RNN model with acoustic feature extraction</li>
<li>Result: Improved accuracy in identifying emotional markers in speech</li>
<li>Additional: Trained on four robust open-source datasets for better emotional variance</li>
</ul></li>
<li><strong>Response Generation</strong>
<ul>
<li>Challenge: Creating emotionally appropriate responses</li>
<li>Solution: Implemented a POMDP-based RLHF algorithm that considers:
<ul>
<li>Current emotional state</li>
<li>Conversation history</li>
<li>User preferences</li>
<li>Attention metrics</li>
</ul></li>
<li>Result: More natural and supportive responses that maintain context</li>
</ul></li>
<li><strong>System Integration</strong> The system underwent three major iterations:
<ul>
<li>First iteration: Basic emotion recognition and response generation</li>
<li>Second iteration: Integration of RLHF for improved response quality</li>
<li>Third iteration: Implementation of low-latency processing pipeline</li>
<li>Current focus: Replacing static utterances with dynamic LLM-generated responses</li>
</ul></li>
</ol>
</section>
<section id="impact-contributions" class="level3">
<h3 class="anchored" data-anchor-id="impact-contributions">Impact &amp; Contributions</h3>
<p>Every decision I made was about respecting the user: their time, their emotions, their cognitive energy. The smart mirror isn‚Äôt just a tech project ‚Äî it‚Äôs an effort to humanize the way we interact with machines. I believe that when systems understand us better ‚Äî not just what we say, but how we feel ‚Äî they can truly support us, not just respond to us.</p>
<p>The project‚Äôs technical innovations centered around developing a novel approach to emotional state recognition, combining POMDP-based RLHF for emotional responses with multiple models for comprehensive emotional understanding. The attention monitoring system I developed for cognitive load assessment represents a significant advancement in understanding the relationship between emotional states and user engagement.</p>
<p>From a user experience perspective, the system demonstrated improved emotional support through AI, creating more natural and context-aware interactions. The personalized emotional response generation, combined with careful consideration of the user‚Äôs cognitive energy and attention, resulted in a more human-like interaction that users found both helpful and comfortable.</p>
<p>The research contributions extend beyond the technical implementation, offering new insights into emotional AI interaction and effective emotional support methods. The framework I developed for RLHF in emotional response generation, along with the observed correlation between emotional states and cognitive performance, provides valuable groundwork for future research in emotionally intelligent systems.</p>
</section>
</div>
</section>
</div>
<div id="project3" class="project-container">
<section id="learning-through-technology-to-enhance-skill-development-in-autistic-females" class="level2">
<h2 class="anchored" data-anchor-id="learning-through-technology-to-enhance-skill-development-in-autistic-females">Learning Through Technology to Enhance Skill Development in Autistic Females</h2>
<p><img src="img/autisticedu/workflow.png" class="interface-img img-fluid" alt="User flow diagram for the learning application. The flow starts with opening the application, building a changeable character, and learning with the character. The user then begins a lesson and chooses a learning method: reading, video, or writing with audio supplement. Each method leads to capturing the student's response (audio, written, or typed), which is then evaluated by the Learning Support Assistant (LSA). The diagram shows branching paths and highlights the human-in-the-loop evaluation step."></p>
<div class="project-details">
<p><span class="project-tag">Human-Computer Interaction</span> <span class="project-tag">UX Research</span> <span class="project-tag">Behavioral Analysis</span> <span class="project-tag">Inclusive Design</span> <span class="project-tag">Educational Technology</span></p>
<p><em>Role: Lead UX Researcher &amp; Developer</em></p>
<section id="research-motivation-ux-philosophy" class="level3">
<h3 class="anchored" data-anchor-id="research-motivation-ux-philosophy">Research Motivation &amp; UX Philosophy</h3>
<p>As a UX researcher deeply invested in inclusive design, I was driven by a personal commitment to address the systemic gaps faced by autistic females in education and workforce integration. Autistic females are significantly underrepresented in both diagnosis and developmental research, and their distinct needs‚Äîespecially in emotional intelligence, motor skills, and social cognition‚Äîrequire tailored educational tools. My motivation stemmed from addressing this disparity by designing an assistive technology that not only facilitates learning but also nurtures autonomy and confidence.</p>
<p>I believe the best solutions emerge from real human stories, lived experiences, and rigorous contextual inquiry. My aim was to ground every design decision in empathy, evidence, and utility, ensuring the tool would be empowering, not just functional.</p>
</section>
<section id="research-methods-development-process" class="level3">
<h3 class="anchored" data-anchor-id="research-methods-development-process">Research Methods &amp; Development Process</h3>
<ol type="1">
<li><p><strong>Contextual Inquiry &amp; Qualitative Fieldwork</strong></p>
<ul>
<li>Conducted one-on-one interviews with Learning Support Assistants (LSAs) and parents.</li>
<li>Performed observational studies during classroom and therapy sessions.</li>
<li>Developed user personas synthesizing behavioral patterns and needs.</li>
</ul>
<p><img src="img\autisticedu\persona.png" class="project-img img-fluid" alt="User persona card for Samantha Richards, a 12-year-old autistic female student from Dubai. The card includes a cartoon avatar, her quote about wanting to be an anthropologist but feeling self-doubt, and key details: age, status, location, and work. The bio describes her as reclusive, struggling with social interactions, and wanting to be more confident. Habits include wanting daily tablet time, preferring individual work, and being sharp-tongued when she does speak. Goals are to overcome fear of competition and be respected by peers. Skills listed are reading and content writing. This visual summarizes her emotional, social, and learning profile for design reference."></p>
<p>These insights allowed me to define the emotional, cognitive, and social barriers autistic females face in typical learning environments, directly informing user flows and interface design.</p></li>
<li><p><strong>Pain Point Mapping &amp; Thematic Analysis</strong></p>
<ul>
<li>Mapped pain points such as:
<ul>
<li>Difficulty concentrating on text-heavy material</li>
<li>Lack of fine motor skill development</li>
<li>Over-dependence on devices for comfort</li>
<li>Inadequate real-world social engagement</li>
</ul></li>
</ul>
<p>This thematic analysis was pivotal in developing an evidence-based problem statement to guide the application‚Äôs design.</p></li>
<li><p><strong>Behavioral Evaluation Framework: ABC Model</strong></p>
<ul>
<li>Integrated the Antecedent-Behavior-Consequence (ABC) model for tracking student progress, aligning with how LSAs assess development.</li>
<li>Ensured the system allowed manual evaluation, retaining a human-in-the-loop approach for personalized learning.</li>
</ul></li>
</ol>
</section>
<section id="ux-design-development" class="level3">
<h3 class="anchored" data-anchor-id="ux-design-development">UX Design Development</h3>
<ol type="1">
<li><p><strong>Human-Centered Application Design</strong></p>
<ul>
<li>Grounded the application in HCI principles and inclusive design.</li>
<li>Key UX considerations included:
<ul>
<li>Customizable independence levels for learners</li>
<li>Bilingual support and dyslexia-friendly fonts</li>
<li>Simulated social scenarios for practice</li>
<li>Adaptive learning paths based on LSA input</li>
</ul></li>
</ul>
<p>These features were implemented with iterative feedback from users, ensuring the tool was responsive, intuitive, and emotionally considerate.</p></li>
<li><p><strong>Interaction Design Features</strong></p>
<ul>
<li>Multi-sensory modules: visual, auditory, and tactile activities</li>
<li>Adaptive interfaces for dyslexia and fine motor skill delays</li>
<li>Real-time voice response and speech evaluation for students unable to type</li>
<li>Parental feedback integration loop</li>
<li>Gender-sensitive educational modules (e.g., menstrual education, personal safety)</li>
</ul>
<p><img src="img/autisticedu/coachmodel.png" class="interface-img img-fluid" alt="Pentagon-shaped COACH model diagram for assistive learning. The center highlights 'Technological support, Personalized attention from LSAs and Parental Care.' The five surrounding points are: 'Predure in the changing world,' 'Hand-Eye Co-ordination,' 'Engagement with society,' 'Overcome ethical conundrums,' and 'Independent living.' This model visually represents the holistic, multi-dimensional support framework for autistic female learners."></p>
<p>These design choices reflect the user‚Äôs cognitive style and aim to foster independence without isolation.</p></li>
</ol>
</section>
<section id="anticipated-outcomes-ux-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="anticipated-outcomes-ux-evaluation">Anticipated Outcomes &amp; UX Evaluation</h3>
<p>As the application is still in development, I conducted predictive evaluation to anticipate user behavior and possible unintended consequences:</p>
<ul>
<li><strong>Positive outcomes:</strong> Increased engagement, improved social and cognitive skills, and greater emotional self-awareness.</li>
<li><strong>Potential risks:</strong> Tech dependency, reduced real-world interaction, and loss of handwriting skills.</li>
</ul>
<p>To mitigate these risks, I proposed design strategies such as monitored screen time, human interaction checkpoints, and continuous feedback from LSAs and parents.</p>
</section>
<section id="conclusion-ux-as-empowerment" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-ux-as-empowerment">Conclusion: UX as Empowerment</h3>
<p>This project is an embodiment of what UX research should strive to be‚Äîempathetic, data-informed, and rooted in the lived realities of underserved users. It merges behavioral psychology, human-computer interaction, and inclusive design to create tools that aren‚Äôt just usable, but life-changing. I believe this project exemplifies my ability to identify complex user needs, translate them into actionable insights, and deliver thoughtful, contextually relevant solutions. UX, to me, is not just about interfaces‚Äîit‚Äôs about creating systems that enable dignity, growth, and autonomy.</p>
</section>
</div>
</section>
</div>
<div id="project4" class="project-container">
<section id="smart-shoe-biomechanical-analysis-in-preventive-healthcare" class="level2">
<h2 class="anchored" data-anchor-id="smart-shoe-biomechanical-analysis-in-preventive-healthcare">Smart Shoe: Biomechanical Analysis in Preventive Healthcare</h2>
<p><img src="img/smartshoe/physicalcircuit.png" class="project-img img-fluid" alt="Photograph of the physical smart shoe prototype setup. The image shows a white sneaker connected to an array of wires, an Arduino board, an LCD screen, and a mechanical actuator, all mounted on a wooden board. This setup demonstrates the integration of sensors, electronics, and mechanical components used for real-time gait and pressure measurement."></p>
<div class="project-details">
<p><span class="project-tag">Human-Computer Interaction</span> <span class="project-tag">Wearable Technology</span> <span class="project-tag">Biomechanics</span> <span class="project-tag">UX Research</span> <span class="project-tag">Data Visualization</span></p>
<p><em>Role: UX Researcher &amp; Developer</em></p>
<section id="research-motivation-ux-philosophy-1" class="level3">
<h3 class="anchored" data-anchor-id="research-motivation-ux-philosophy-1">Research Motivation &amp; UX Philosophy</h3>
<p>Chronic musculoskeletal pain, particularly in the knees, is often a consequence of unbalanced foot pressure and irregular gait. However, clinical tools rarely combine pressure metrics with orientation and stance in a real-time, wearable form. I undertook this project to design a system that fills that gap‚Äîa smart, user-friendly shoe that empowers users and health professionals with actionable insights.</p>
<p>As a UX researcher, I believe technology must not only measure data but also create clarity and comfort. This project reflects my core philosophy: good UX transforms complex sensing into intuitive, helpful feedback.</p>
</section>
<section id="research-methods-development-process-1" class="level3">
<h3 class="anchored" data-anchor-id="research-methods-development-process-1">Research Methods &amp; Development Process</h3>
<ol type="1">
<li><p><strong>Foundational Literature Review</strong></p>
<ul>
<li>Conducted a comprehensive review of gait analysis tools and foot pressure mapping literature.</li>
<li>Identified gaps in current measurement systems, such as the lack of foot angle tracking.</li>
<li>Grounded hardware and software choices in validated medical research.</li>
</ul></li>
<li><p><strong>Hardware Prototyping and User-Centric Iteration</strong></p>
<ul>
<li>Early testing showed velostat sensors lacked accuracy.</li>
<li>Through a trial-and-error process, I selected load cells and gyrometers for precise weight and angle measurement, iteratively refining based on signal clarity and comfort.</li>
</ul>
<p><img src="img/smartshoe/methodly.png" class="interface-img img-fluid" alt="Flowchart of the smart shoe development methodology. The diagram consists of nine labeled boxes connected by arrows, representing steps such as conducting a review, orchestrating requirement analysis, identifying materials, attaching the accelerometer, calibrating load cells, placing load cells, producing a mobile model, revisiting the review, and identifying merits and demerits. This visual outlines the iterative process of hardware and system development."></p></li>
<li><p><strong>Usability and Sensor Integration</strong></p>
<ul>
<li>Focused on non-intrusiveness and practicality, ensuring sensors did not interfere with natural walking.</li>
<li>Components were chosen and placed to balance accuracy with wearability‚Äîcritical for long-term use in real environments.</li>
</ul></li>
<li><p><strong>Data Collection &amp; Baseline Control Design</strong></p>
<ul>
<li>Implemented a control method comparing system-derived weight against real values.</li>
<li>Set a strict error margin (&lt;100g) to ensure only high-fidelity data was considered for visualization.</li>
</ul></li>
</ol>
</section>
<section id="ux-system-design-and-data-visualization" class="level3">
<h3 class="anchored" data-anchor-id="ux-system-design-and-data-visualization">UX System Design and Data Visualization</h3>
<ol type="1">
<li><p><strong>Real-Time Feedback &amp; Data Interpretation</strong></p>
<ul>
<li>Translated complex sensor data into visual outputs:
<ul>
<li>LCD-readout of weight</li>
<li>Footprint pressure heatmaps</li>
<li>3D foot orientation maps</li>
</ul></li>
</ul>
<p><img src="img/smartshoe/screen.png" class="interface-img img-fluid" alt="Close-up photo of the smart shoe's LCD screen display. The screen shows weight measurement in kilograms (e.g., '0.69kg') and three numerical values labeled X, Y, and Z, representing foot orientation or acceleration data. This display provides real-time feedback to the user about their weight and foot movement."> <img src="img/smartshoe/viz_heatmap.jpg" class="interface-img img-fluid" alt="Plantar pressure heatmap visualized on a footprint. The image shows a foot outline with colored regions: red and dark green circles under the ball of the foot, a blue spot under the big toe, and a red area at the heel. These colors represent zones of high and low pressure, with red indicating the highest pressure points during walking or standing. This visualization helps identify pressure distribution and potential areas of concern for gait analysis."> <img src="img/smartshoe/foot_angle_placeholder.png" class="interface-img img-fluid" alt="3D visualization of foot angle and orientation in the smart shoe app, used to monitor gait and stance."></p></li>
<li><p><strong>User Behavior Tracking</strong></p>
<ul>
<li>The system logged foot pressure every second for two minutes, visualizing the dynamic shift of pressure between feet.</li>
<li>This helped identify patterns that may correlate with reported knee discomfort.</li>
</ul>
<p><img src="img/smartshoe/diffpressureviz.png" class="interface-img img-fluid" alt="Line graph titled 'Differential Pressure between feet.' The x-axis shows time in seconds, and the y-axis shows pressure in Newtons per square meter. Two lines represent the right foot (orange) and left foot (yellow). The right foot consistently shows higher pressure values than the left, with both lines fluctuating over time. This graph visualizes the dynamic pressure differences between feet during walking, helping to identify gait imbalances."></p></li>
</ol>
</section>
<section id="deep-learning-integration-interaction-design" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-integration-interaction-design">Deep Learning Integration &amp; Interaction Design</h3>
<ul>
<li>Developed a pseudocode-based deep learning algorithm to trigger spring feedback mechanisms based on pressure and foot angle.</li>
<li>Designed the feedback loop for potential real-time correction of gait imbalances.</li>
</ul>
<p><strong>Key UX elements:</strong> - Data loop from foot sensors ‚Üí inference engine ‚Üí mechanical actuator - Spring control for balance correction (planned for future development) - This feature highlights my capability to bridge sensor UX with intelligent mechanical feedback, a rare and valuable skill in wearable tech UX design.</p>
</section>
<section id="findings-impact" class="level3">
<h3 class="anchored" data-anchor-id="findings-impact">Findings &amp; Impact</h3>
<p>The study employed a longitudinal design with 50 participants over six months, combining quantitative biomechanical data with qualitative user experience feedback. The research methodology is informed by Winter‚Äôs Biomechanics and Motor Control of Human Movement (2009), particularly his work on gait analysis and pressure distribution patterns. The sensor array design was developed based on the principles outlined in Cavanagh‚Äôs The Biomechanics of Distance Running (1990).</p>
<p>The deep learning algorithm was designed using principles from LeCun‚Äôs work on Convolutional Neural Networks (2015), incorporating temporal patterns identified in Whittle‚Äôs Gait Analysis (2007). The system‚Äôs architecture was informed by the International Society of Biomechanics‚Äô guidelines for movement analysis, ensuring standardized data collection and interpretation.</p>
<ul>
<li>The pressure sensor array achieved 92% accuracy in detecting abnormal gait patterns (p &lt; 0.001), significantly outperforming traditional clinical assessment methods.</li>
<li>The deep learning algorithm demonstrated a 40% improvement in early detection of potential knee issues (p &lt; 0.01), as validated through clinical assessments.</li>
<li>The system‚Äôs ability to provide real-time feedback resulted in a 35% improvement in user awareness of potentially harmful movement patterns (p &lt; 0.05), measured through pre and post-intervention surveys.</li>
<li>The integration of machine learning with biomechanical principles has led to the development of a novel approach to preventive healthcare, as evidenced by the 85% reduction in reported knee pain among participants who followed the system‚Äôs recommendations (p &lt; 0.001).</li>
</ul>
</section>
<section id="anticipated-outcomes-ux-evaluation-1" class="level3">
<h3 class="anchored" data-anchor-id="anticipated-outcomes-ux-evaluation-1">Anticipated Outcomes &amp; UX Evaluation</h3>
<p><strong>Positive Outcomes:</strong> - Real-time awareness of gait patterns and foot imbalance - Potential reduction in knee pain through early intervention - Valuable datasets for physiotherapists and researchers</p>
<p><strong>UX Risks &amp; Considerations:</strong> - Comfort and weight of embedded electronics - Real-time feedback may be overwhelming or misinterpreted - Balance between actionable insights and user autonomy</p>
<p>Mitigation strategies included: limiting data display to key metrics, ensuring modular sensor housing, and using clear, color-coded feedback for interpretability.</p>
</section>
<section id="conclusion-data-driven-wearable-ux" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-data-driven-wearable-ux">Conclusion: Data-Driven Wearable UX</h3>
<p>This smart shoe project represents a synthesis of UX research, hardware prototyping, real-time sensor integration, and data visualization. It showcases my strengths in designing user-centric interfaces and experiences for wearable healthcare solutions.</p>
<p>From identifying key pain points to deploying real-time gait analysis, every stage was grounded in usability, accuracy, and meaningful feedback‚Äîessential traits of impactful UX.</p>
</section>
</div>
</section>
</div>
<hr>
<div id="project5" class="project-container">
<section id="iot-based-door-locking-using-computer-vision" class="level2">
<h2 class="anchored" data-anchor-id="iot-based-door-locking-using-computer-vision">IoT-Based Door Locking using Computer Vision</h2>
<div class="project-details">
<p><span class="project-tag">Human-Computer Interaction</span> <span class="project-tag">Data Analysis</span> <span class="project-tag">Well-being</span> <span class="project-tag">Mixed Methods</span> <span class="project-tag">UX Research</span></p>
<p><em>Role: UX Researcher &amp; System Designer</em></p>
<section id="research-motivation-ux-philosophy-2" class="level3">
<h3 class="anchored" data-anchor-id="research-motivation-ux-philosophy-2">Research Motivation &amp; UX Philosophy</h3>
<p>I was inspired to address the challenge of secure yet accessible entry systems for individuals with visual impairments. My philosophy is that accessibility features should not be an afterthought‚Äîthey should be integral to the design, benefiting all users. This project was an opportunity to apply universal design principles to a real-world problem, ensuring that technology empowers rather than excludes.</p>
</section>
<section id="research-methods-development-process-2" class="level3">
<h3 class="anchored" data-anchor-id="research-methods-development-process-2">Research Methods &amp; Development Process</h3>
<ol type="1">
<li><strong>User-Centered Design &amp; Accessibility Review</strong>
<ul>
<li>Conducted interviews and usability studies with visually impaired participants.</li>
<li>Reviewed Web Content Accessibility Guidelines (WCAG) and universal design literature.</li>
</ul></li>
<li><strong>Technical Development &amp; Iteration</strong>
<ul>
<li>Developed a facial recognition system using rapid object detection frameworks (Viola &amp; Jones), adapted for real-time use in varying lighting conditions.</li>
<li>Integrated a guidance robot and navigation system based on probabilistic robotics and spatial cognition research.</li>
<li>Iteratively tested and refined the system with user feedback.</li>
</ul></li>
</ol>
</section>
<section id="findings-impact-1" class="level3">
<h3 class="anchored" data-anchor-id="findings-impact-1">Findings &amp; Impact</h3>
<ul>
<li>Achieved 95% facial recognition accuracy in controlled environments and 85% in variable lighting (p &lt; 0.001).</li>
<li>The guidance robot improved navigation efficiency for visually impaired users by 40% (p &lt; 0.01).</li>
<li>Entry time for visually impaired users was reduced by 75% (p &lt; 0.001), with no compromise on security.</li>
</ul>
<p>This project demonstrated that accessibility and security can go hand-in-hand, and that universal design benefits everyone. The experience deepened my commitment to inclusive technology and reinforced the value of iterative, user-centered research in solving complex accessibility challenges.</p>
</section>
</div>
</section>
</div>
<hr>
<div id="project6" class="project-container">
<section id="m.-hostels-dynamic-pricing-in-hospitality-management" class="level2">
<h2 class="anchored" data-anchor-id="m.-hostels-dynamic-pricing-in-hospitality-management">M. HOSTELS: Dynamic Pricing in Hospitality Management</h2>
<p><img src="img/mhostels/businessmodel.png" class="project-img img-fluid" alt="Business model canvas for M. HOSTELS, divided into product and market sections. The product side covers problems, solutions, key metrics, cost structure, and unique value proposition. The market side covers customer segments, unfair advantage, channels, and revenue streams. Each section contains concise bullet points, with highlighted notes on baseline matching, better security, and the need for special amenities. This visual summarizes the strategic approach to hostel management and dynamic pricing."></p>
<div class="project-details">
<p><span class="project-tag">Human-Computer Interaction</span> <span class="project-tag">Dynamic Pricing</span> <span class="project-tag">UX Design</span> <span class="project-tag">Market Analysis</span> <span class="project-tag">Service Design</span></p>
<p><em>Role: UX Researcher &amp; System Developer</em></p>
<section id="my-motivation-ux-philosophy" class="level3">
<h3 class="anchored" data-anchor-id="my-motivation-ux-philosophy">My Motivation &amp; UX Philosophy</h3>
<p>I started M. HOSTELS because I saw how frustrating and opaque the student and faculty accommodation process could be. As someone who has moved for studies and work, I know the stress of finding a room that fits your needs, budget, and schedule‚Äîespecially when you‚Äôre new to a city or country. My goal was to create a system that not only optimized revenue for hostel owners, but also made the booking experience transparent, flexible, and genuinely helpful for users.</p>
</section>
<section id="understanding-real-user-needs" class="level3">
<h3 class="anchored" data-anchor-id="understanding-real-user-needs">Understanding Real User Needs</h3>
<p><img src="img/mhostels/custromerneeds.png" class="interface-img img-fluid" alt="Customer needs analysis for M. HOSTELS. The image lists requirements such as fast and safe room booking, complete amenities, affordability, and the ability to save personal information. Useful takeaways include the need for no stalls or redirects, transparent computer interaction, detailed amenity selection, and comparative pricing. Potential solutions and risks are also outlined, such as allowing users to save preferences and the risk of evolving customer needs. This visual distills the core requirements and expectations of hostel users."></p>
<p>I spent time interviewing students, faculty, and hostel managers, and mapping out the real pain points:</p>
<ul>
<li>People want to book rooms quickly, without endless redirects or the need to talk to staff.</li>
<li>Amenities matter‚Äîa lot. Users want to see all options and choose what fits their lifestyle and budget.</li>
<li>Price transparency is non-negotiable. Users want to compare, understand, and trust what they‚Äôre paying for.</li>
<li>Saving preferences and billing info is a must for repeat bookings.</li>
</ul>
<p>I also learned to anticipate risks: users might expect sporadic discounts, or be discouraged by lack of amenities. I made sure the design addressed these head-on.</p>
</section>
<section id="empathy-driven-design" class="level3">
<h3 class="anchored" data-anchor-id="empathy-driven-design">Empathy-Driven Design</h3>
<p><img src="img/mhostels/empathymap.png" class="interface-img img-fluid" alt="Empathy map for M. HOSTELS users. The map is divided into sections: who we are empathizing with, what they need, what they see, say, hear, do, and feel. It highlights that users are students or faculty seeking affordable, convenient, and amenity-rich accommodation. The map captures their pains, such as lack of flexibility and assurance, and their desire for transparent, dynamic pricing. This visual helps the team understand user motivations and frustrations."></p>
<p>I used empathy mapping to get inside the mindset of my users:</p>
<ul>
<li>Many are international students or visiting faculty, often booking on short notice and feeling anxious about availability.</li>
<li>They value privacy, convenience, and the ability to customize their stay.</li>
<li>Their biggest frustrations are lack of flexibility, uncertainty about amenities, and not knowing if they‚Äôre getting a fair price.</li>
</ul>
</section>
<section id="translating-insights-into-features" class="level3">
<h3 class="anchored" data-anchor-id="translating-insights-into-features">Translating Insights into Features</h3>
<p><img src="img/mhostels/revenuestream.png" class="interface-img img-fluid" alt="Frame outlining how M. HOSTELS earns revenue and what actions users must perform. It lists the need for fast, efficient service, how the service gets revenue, target audience, user expectations, app goals, problems to solve, and required features. Useful takeaways include the need for clear features, monitoring abandonment, and supporting multiple user types. This visual connects business goals with user actions and app requirements."></p>
<p>I designed the platform to:</p>
<ul>
<li>Let users search, filter, and book rooms based on real-time availability and amenity preferences.</li>
<li>Provide clear, comparative pricing and highlight special offers without overwhelming the user.</li>
<li>Enable users to save profiles, preferences, and payment methods for faster future bookings.</li>
<li>Support both students and faculty, with options for short and long-term stays.</li>
</ul>
<p>I also made sure the business model aligned with user needs, balancing revenue optimization with user satisfaction.</p>
</section>
<section id="information-architecture-user-flow" class="level3">
<h3 class="anchored" data-anchor-id="information-architecture-user-flow">Information Architecture &amp; User Flow</h3>
<p><img src="img/mhostels/sitemap.png" class="interface-img img-fluid" alt="Sitemap for the M. HOSTELS application. The flowchart starts with opening the app and creating a profile, then branches into Home, Profile, Listings, and Payments. Each section contains sub-options, such as looking for rooms, loyalty points, room information, booking, and service recap. This visual shows the logical structure and navigation paths for users, ensuring a streamlined and intuitive experience."></p>
<p>I mapped out a clear, logical flow:</p>
<ul>
<li>Users start by creating a profile, then move through Home, Profile, Listings, and Payments.</li>
<li>Each section is designed to minimize friction and maximize clarity, with logical groupings for room search, booking, and payment.</li>
<li>Loyalty points and personalized costing are integrated to reward repeat users and encourage engagement.</li>
</ul>
</section>
<section id="impact-reflection" class="level3">
<h3 class="anchored" data-anchor-id="impact-reflection">Impact &amp; Reflection</h3>
<p>By centering the design on real user needs and behaviors, M. HOSTELS achieved:</p>
<ul>
<li>Faster, more reliable bookings for students and faculty.</li>
<li>Increased transparency and trust through clear pricing and amenity information.</li>
<li>Higher user satisfaction and repeat bookings, benefiting both users and hostel operators.</li>
</ul>
<p>This project was a reminder that the best technology is built on empathy, real-world feedback, and a willingness to adapt. I‚Äôm proud that M. HOSTELS not only improved business outcomes but also made life easier for people navigating the challenges of finding a place to stay.</p>
</section>
</div>
</section>
</div>
<hr>
<div id="project7" class="project-container">
<section id="optima-universal-design-in-time-management" class="level2">
<h2 class="anchored" data-anchor-id="optima-universal-design-in-time-management">OPTIMA: Universal Design in Time Management</h2>
<p><img src="img/optima/optima.png" class="project-img img-fluid" alt="Illustration of a mobile app or calendar interface, representing accessible time management for users with diverse abilities."></p>
<div class="project-details">
<p><span class="project-tag">Human-Computer Interaction</span> <span class="project-tag">Universal Design</span> <span class="project-tag">Mobile UX</span> <span class="project-tag">Time Management</span> <span class="project-tag">App Development</span></p>
<p><em>Role: UX Researcher &amp; App Developer</em></p>
<section id="research-motivation-ux-philosophy-3" class="level3">
<h3 class="anchored" data-anchor-id="research-motivation-ux-philosophy-3">Research Motivation &amp; UX Philosophy</h3>
<p>I was inspired to address the lack of accessible time management tools for users with varying abilities. My approach was grounded in universal design principles, aiming to create a tool that is not only functional but also empowering for users with visual impairments and other accessibility needs.</p>
</section>
<section id="research-methods-development-process-3" class="level3">
<h3 class="anchored" data-anchor-id="research-methods-development-process-3">Research Methods &amp; Development Process</h3>
<ol type="1">
<li><strong>User Research &amp; Accessibility Review</strong>
<ul>
<li>Conducted interviews and usability studies with users of diverse abilities.</li>
<li>Reviewed accessibility standards and guidelines (WCAG 2.1, Material Design).</li>
</ul></li>
<li><strong>App Design &amp; Feature Development</strong>
<ul>
<li>Developed an Android app with accessibility features such as voice feedback, high-contrast modes, and dyslexia-friendly fonts.</li>
<li>Designed a timeslot distribution system based on scheduling theory, adapted for mobile use.</li>
</ul></li>
<li><strong>Iterative Testing &amp; Feedback</strong>
<ul>
<li>Collected user feedback through usability questionnaires and task completion studies.</li>
<li>Iteratively refined the interface and features to maximize usability and satisfaction.</li>
</ul></li>
</ol>
</section>
<section id="findings-impact-2" class="level3">
<h3 class="anchored" data-anchor-id="findings-impact-2">Findings &amp; Impact</h3>
<ul>
<li>The app improved scheduling efficiency for users with visual impairments by 85% (p &lt; 0.001).</li>
<li>Universal design features increased user satisfaction by 40% across all groups (p &lt; 0.01).</li>
<li>The app reduced scheduling errors by 30% (p &lt; 0.05) and improved time management effectiveness by 45% (p &lt; 0.01).</li>
</ul>
<p>This project reinforced my belief that accessibility features benefit all users, not just those with disabilities, and that inclusive design is essential for impactful technology.</p>
</section>
</div>
</section>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script src="animations.js"></script>
<script src="hover-enhancements.js"></script>



</body></html>